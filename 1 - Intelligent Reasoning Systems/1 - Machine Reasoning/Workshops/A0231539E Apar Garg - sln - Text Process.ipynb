{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Workshop] Textual Knowledge Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ws_img_001.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Package Installation (one time job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install pandas\n",
    "# !pip install gingerit\n",
    "# !pip install gensim==3.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from gingerit.gingerit import GingerIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "#nltk.download('word2vec_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_casing(sentence):\n",
    "    # Quiz: How to implement this function without using str.lower()?\n",
    "    new_sentence = sentence.lower()\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_casing_spacy(sentence):\n",
    "   \n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Abbreviation expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_abbriviation(sentence):\n",
    "    replacement_patterns = [\n",
    "        (r'won\\'t', 'will not'),\n",
    "        (r'can\\'t', 'cannot'),\n",
    "        (r'i\\'m', 'i am'),\n",
    "        (r'ain\\'t', 'is not'),\n",
    "        (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "        (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "        (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "        (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "        (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "        (r'(\\w+)\\'d', '\\g<1> would')]\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in replacement_patterns]\n",
    "\n",
    "    new_sentence = sentence\n",
    "    for (pattern, repl) in patterns:\n",
    "        (new_sentence, count) = re.subn(pattern, repl, new_sentence)\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load('wiki.kv')\n",
    "def expand_abbriviation_spacy(sentence):\n",
    "    \n",
    "    final={}\n",
    "    \n",
    "    for word1 in sentence.split():\n",
    "        \n",
    "        word=word1.replace('.','')\n",
    "        \n",
    "        if(len(word)<=3 and word==word.upper()):\n",
    "            try:\n",
    "                l = [i[0] for i in model.most_similar(word, topn=200)]\n",
    "                \n",
    "                for i in l:\n",
    "\n",
    "                    if(len(i.split('-'))==len(word)):\n",
    "                    \n",
    "                        k=''\n",
    "                        for j in i.split('-'):\n",
    "                            try:\n",
    "                                k+=str(j[0])\n",
    "                            except:\n",
    "                                \n",
    "                                pass\n",
    "                       \n",
    "                            \n",
    "                        if(word==k):\n",
    "                            final[word1]= i.replace('-',' ')\n",
    "                            break\n",
    "                      \n",
    "\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    s=sentence.split()\n",
    "    \n",
    "    for i in range(len(s)):\n",
    "        try:\n",
    "            s[i] = final[s[i]]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    sentence = \" \".join(s)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_removal(sentence):\n",
    "    # Remove the all the punctuations except '\n",
    "    new_sentence = re.sub(',|!|\\?|\\\"|<|>|\\(|\\)|\\[|\\]|\\{|\\}|@|#|\\+|\\=|\\-|\\_|~|\\&|\\*|\\^|%|\\||\\$|/|`|\\.|\\'',\n",
    "                          '', sentence,count=0, flags=0)\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_removal_spacy(sentence):\n",
    "    # Remove the all the punctuations except '\n",
    "    \n",
    "    sentence = nlp(sentence)\n",
    "    \n",
    "    sentence = [token.text for token in sentence if token.is_alpha==True or token.text==\"'\"]\n",
    "    \n",
    "    sentence = \" \".join(sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(sentence):\n",
    "    new_sentence = nltk.word_tokenize(sentence)\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correction(sentence):\n",
    "    \n",
    "    result = GingerIt().parse(sentence)\n",
    "\n",
    "    sentence = result['result']\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_removal(sentence):\n",
    "    #stoplist = stopwords.words('english')\n",
    "     \n",
    "    with open('./stopwords.txt') as file:\n",
    "        stoplist = [stopword.replace('\\n', '').lower() for stopword in file.readlines()]\n",
    "    \n",
    "    new_sentence = [word for word in sentence if word not in stoplist]\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_removal_spacy(sentence):\n",
    "    \n",
    "    sentence = nlp(sentence)\n",
    "    \n",
    "    sentence = [token.text for token in sentence if token.is_stop==False]\n",
    "    \n",
    "    sentence = \" \".join(sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    pack = nltk.pos_tag([word])\n",
    "    tag = pack[0][1]\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatization(sentence):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    new_sentence = [lemmatizer.lemmatize(word, get_wordnet_pos(word) or wordnet.NOUN) for word in sentence]\n",
    "\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_spacy(sentence):\n",
    "    \n",
    "    sentence = nlp(sentence)\n",
    "    \n",
    "    sentence = [word.lemma_ for word in sentence]\n",
    "    \n",
    "    sentence = \" \".join(sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. Integrate all the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(raw_sentence):\n",
    "    sentence = lower_casing(raw_sentence)\n",
    "    sentence = expand_abbriviation(sentence)\n",
    "    sentence = punctuation_removal(sentence)\n",
    "    sentence = tokenization(sentence)\n",
    "    sentence = stopword_removal(sentence)\n",
    "    sentence = lemmatization(sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preprocessing(raw_sentence):\n",
    "    sentence = lower_casing_spacy(raw_sentence)\n",
    "    sentence = expand_abbriviation_spacy(sentence)\n",
    "    sentence = punctuation_removal_spacy(sentence)\n",
    "    sentence = stopword_removal_spacy(sentence)\n",
    "    sentence = lemmatization_spacy(sentence)\n",
    "\n",
    "    return sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Lets have a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./questionbase_raw.txt') as file:\n",
    "    raw_sentences = [sentence.replace('\\n', '') for sentence in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Hello \n",
      "\n",
      "NLTK:  [] \n",
      "\n",
      "SPACY:  ['hello'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  {'hello'} \n",
      "\n",
      "****************************************************************************************************\n",
      "2 Hello, I am ASD knowledge bot. Feel free to ask me anything about autism spectrum disorder (ASD). \n",
      "\n",
      "NLTK:  ['asd', 'knowledge', 'bot', 'feel', 'free', 'autism', 'spectrum', 'disorder', 'asd'] \n",
      "\n",
      "SPACY:  ['hello', 'asd', 'knowledge', 'bot', 'feel', 'free', 'ask', 'autism', 'spectrum', 'disorder', 'asd'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  {'ask', 'hello'} \n",
      "\n",
      "****************************************************************************************************\n",
      "3 What is definition of Autistic Spectrum Disorder? \n",
      "\n",
      "NLTK:  ['definition', 'autistic', 'spectrum', 'disorder'] \n",
      "\n",
      "SPACY:  ['definition', 'autistic', 'spectrum', 'disorder'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  set() \n",
      "\n",
      "****************************************************************************************************\n",
      "4 Autism, or autism spectrum disorder (ASD), refers to a broad range of conditions characterized by challenges with social skills, repetitive behaviors, speech and nonverbal communication. According to the Centers for Disease Control, autism affects an estimated 1 in 54 children in the United States today. \n",
      "\n",
      "NLTK:  ['autism', 'autism', 'spectrum', 'disorder', 'asd', 'refers', 'broad', 'range', 'condition', 'characterize', 'challenge', 'social', 'skill', 'repetitive', 'behavior', 'speech', 'nonverbal', 'communication', 'center', 'disease', 'control', 'autism', 'estimate', '1', '54', 'child', 'united'] \n",
      "\n",
      "SPACY:  ['autism', 'autism', 'spectrum', 'disorder', 'asd', 'refer', 'broad', 'range', 'condition', 'characterize', 'challenge', 'social', 'skill', 'repetitive', 'behavior', 'speech', 'nonverbal', 'communication', 'accord', 'center', 'disease', 'control', 'autism', 'affect', 'estimate', 'child', 'unite', 'state', 'today'] \n",
      "\n",
      "NLTK extra tokens:  {'refers', 'united', '54', '1'} \n",
      "\n",
      "SPACY extra tokens::  {'refer', 'state', 'today', 'unite', 'accord', 'affect'} \n",
      "\n",
      "****************************************************************************************************\n",
      "5 What are the symptoms of Autistic Spectrum Disorder? \n",
      "\n",
      "NLTK:  ['symptom', 'autistic', 'spectrum', 'disorder'] \n",
      "\n",
      "SPACY:  ['symptom', 'autistic', 'spectrum', 'disorder'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  set() \n",
      "\n",
      "****************************************************************************************************\n",
      "6 Making little or inconsistent eye contact.  \n",
      "\n",
      "NLTK:  ['inconsistent', 'eye', 'contact'] \n",
      "\n",
      "SPACY:  ['make', 'little', 'inconsistent', 'eye', 'contact'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  {'little', 'make'} \n",
      "\n",
      "****************************************************************************************************\n",
      "7 Tending not to look at or listen to people. \n",
      "\n",
      "NLTK:  ['tend', 'not', 'listen', 'people'] \n",
      "\n",
      "SPACY:  ['tend', 'look', 'listen', 'people'] \n",
      "\n",
      "NLTK extra tokens:  {'not'} \n",
      "\n",
      "SPACY extra tokens::  {'look'} \n",
      "\n",
      "****************************************************************************************************\n",
      "8 Rarely sharing enjoyment of objects or activities by pointing or showing things to others. \n",
      "\n",
      "NLTK:  ['rarely', 'share', 'enjoyment', 'object', 'activity'] \n",
      "\n",
      "SPACY:  ['rarely', 'share', 'enjoyment', 'object', 'activity', 'point', 'show', 'thing'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  {'point', 'thing', 'show'} \n",
      "\n",
      "****************************************************************************************************\n",
      "9 Failing to, or being slow to, respond to someone calling their name or to other verbal attempts to gain attention. \n",
      "\n",
      "NLTK:  ['fail', 'slow', 'respond', 'call', 'verbal', 'attempt', 'gain', 'attention'] \n",
      "\n",
      "SPACY:  ['fail', 'slow', 'respond', 'call', 'verbal', 'attempt', 'gain', 'attention'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  set() \n",
      "\n",
      "****************************************************************************************************\n",
      "10 Having difficulties with the back and forth of conversation. \n",
      "\n",
      "NLTK:  ['difficulty', 'conversation'] \n",
      "\n",
      "SPACY:  ['have', 'difficulty', 'forth', 'conversation'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  {'forth', 'have'} \n",
      "\n",
      "****************************************************************************************************\n",
      "11 Often talking at length about a favorite subject without noticing that others are not interested or without giving others a chance to respond. \n",
      "\n",
      "NLTK:  ['talk', 'length', 'favorite', 'subject', 'notice', 'not', 'chance', 'respond'] \n",
      "\n",
      "SPACY:  ['talk', 'length', 'favorite', 'subject', 'notice', 'interested', 'give', 'chance', 'respond'] \n",
      "\n",
      "NLTK extra tokens:  {'not'} \n",
      "\n",
      "SPACY extra tokens::  {'interested', 'give'} \n",
      "\n",
      "****************************************************************************************************\n",
      "12 Having facial expressions, movements, and gestures that do not match what is being said. \n",
      "\n",
      "NLTK:  ['facial', 'expression', 'movement', 'gesture', 'not', 'match'] \n",
      "\n",
      "SPACY:  ['have', 'facial', 'expression', 'movement', 'gesture', 'match', 'say'] \n",
      "\n",
      "NLTK extra tokens:  {'not'} \n",
      "\n",
      "SPACY extra tokens::  {'have', 'say'} \n",
      "\n",
      "****************************************************************************************************\n",
      "13 Having an unusual tone of voice that may sound sing-song or flat and robot-like. \n",
      "\n",
      "NLTK:  ['unusual', 'tone', 'voice', 'sound', 'singsong', 'flat', 'robotlike'] \n",
      "\n",
      "SPACY:  ['have', 'unusual', 'tone', 'voice', 'sound', 'sing', 'song', 'flat', 'robot', 'like'] \n",
      "\n",
      "NLTK extra tokens:  {'singsong', 'robotlike'} \n",
      "\n",
      "SPACY extra tokens::  {'song', 'robot', 'sing', 'like', 'have'} \n",
      "\n",
      "****************************************************************************************************\n",
      "14 Having trouble understanding another person’s point of view or being unable to predict or understand other people’s actions. \n",
      "\n",
      "NLTK:  ['trouble', 'understand', 'person', '’', 'view', 'unable', 'predict', 'understand', 'people', '’', 'action'] \n",
      "\n",
      "SPACY:  ['have', 'trouble', 'understand', 'person', 'point', 'view', 'unable', 'predict', 'understand', 'people', 'action'] \n",
      "\n",
      "NLTK extra tokens:  {'’'} \n",
      "\n",
      "SPACY extra tokens::  {'point', 'have'} \n",
      "\n",
      "****************************************************************************************************\n",
      "15 Repeating certain behaviors or having unusual behaviors. For example, repeating words or phrases, a behavior called echolalia. \n",
      "\n",
      "NLTK:  ['repeat', 'behavior', 'unusual', 'behavior', 'repeat', 'phrase', 'behavior', 'call', 'echolalia'] \n",
      "\n",
      "SPACY:  ['repeat', 'certain', 'behavior', 'have', 'unusual', 'behavior', 'example', 'repeat', 'word', 'phrase', 'behavior', 'call', 'echolalia'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  {'word', 'have', 'certain', 'example'} \n",
      "\n",
      "****************************************************************************************************\n",
      "16 Having a lasting intense interest in certain topics, such as numbers, details, or facts. \n",
      "\n",
      "NLTK:  ['last', 'intense', 'topic', 'detail'] \n",
      "\n",
      "SPACY:  ['have', 'last', 'intense', 'interest', 'certain', 'topic', 'number', 'detail', 'fact'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  {'interest', 'number', 'certain', 'have', 'fact'} \n",
      "\n",
      "****************************************************************************************************\n",
      "17 Having overly focused interests, such as with moving objects or parts of objects. \n",
      "\n",
      "NLTK:  ['overly', 'focus', 'move', 'object', 'object'] \n",
      "\n",
      "SPACY:  ['having', 'overly', 'focus', 'interest', 'move', 'object', 'part', 'object'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  {'interest', 'part', 'having'} \n",
      "\n",
      "****************************************************************************************************\n",
      "18 Getting upset by slight changes in a routine. \n",
      "\n",
      "NLTK:  ['upset', 'slight', 'routine'] \n",
      "\n",
      "SPACY:  ['get', 'upset', 'slight', 'change', 'routine'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  {'change', 'get'} \n",
      "\n",
      "****************************************************************************************************\n",
      "19 Being more or less sensitive than other people to sensory input, such as light, noise, clothing, or temperature. \n",
      "\n",
      "NLTK:  ['sensitive', 'people', 'sensory', 'input', 'light', 'noise', 'clothing', 'temperature'] \n",
      "\n",
      "SPACY:  ['sensitive', 'people', 'sensory', 'input', 'light', 'noise', 'clothing', 'temperature'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  set() \n",
      "\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 People with ASD may also experience sleep problems and irritability. Although people with ASD experience many challenges, they may also have many strengths, including: \n",
      "\n",
      "NLTK:  ['people', 'asd', 'experience', 'sleep', 'irritability', 'people', 'asd', 'experience', 'challenge', 'strength', 'include', ':'] \n",
      "\n",
      "SPACY:  ['people', 'asd', 'experience', 'sleep', 'problem', 'irritability', 'people', 'asd', 'experience', 'challenge', 'strength', 'include'] \n",
      "\n",
      "NLTK extra tokens:  {':'} \n",
      "\n",
      "SPACY extra tokens::  {'problem'} \n",
      "\n",
      "****************************************************************************************************\n",
      "21 Being able to learn things in detail and remember information for long periods of time. \n",
      "\n",
      "NLTK:  ['learn', 'detail', 'remember', 'period', 'time'] \n",
      "\n",
      "SPACY:  ['able', 'learn', 'thing', 'detail', 'remember', 'information', 'long', 'period', 'time'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  {'able', 'thing', 'information', 'long'} \n",
      "\n",
      "****************************************************************************************************\n",
      "22 Being strong visual and auditory learners. \n",
      "\n",
      "NLTK:  ['strong', 'visual', 'auditory', 'learner'] \n",
      "\n",
      "SPACY:  ['strong', 'visual', 'auditory', 'learner'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  set() \n",
      "\n",
      "****************************************************************************************************\n",
      "23 Excelling in math, science, music, or art. \n",
      "\n",
      "NLTK:  ['excel', 'math', 'science', 'music', 'art'] \n",
      "\n",
      "SPACY:  ['excel', 'math', 'science', 'music', 'art'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  set() \n",
      "\n",
      "****************************************************************************************************\n",
      "24 What should I do if I got diagnosis the Autistic Spectrum Disorder? \n",
      "\n",
      "NLTK:  ['diagnosis', 'autistic', 'spectrum', 'disorder'] \n",
      "\n",
      "SPACY:  ['get', 'diagnosis', 'autistic', 'spectrum', 'disorder'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  {'get'} \n",
      "\n",
      "****************************************************************************************************\n",
      "25 Consult a doctor and psychologist for early intervention programs. \n",
      "\n",
      "NLTK:  ['consult', 'doctor', 'psychologist', 'intervention', 'program'] \n",
      "\n",
      "SPACY:  ['consult', 'doctor', 'psychologist', 'early', 'intervention', 'program'] \n",
      "\n",
      "NLTK extra tokens:  set() \n",
      "\n",
      "SPACY extra tokens::  {'early'} \n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "i = 1\n",
    "for raw_sentence in raw_sentences:\n",
    "    processed_sentence1 = text_preprocessing(raw_sentence)\n",
    "    processed_sentence2 = my_preprocessing(raw_sentence)\n",
    "    if raw_sentence != 'Q' and raw_sentence != 'A':\n",
    "        print(i, raw_sentence,\"\\n\")\n",
    "        print(\"NLTK: \",processed_sentence1,\"\\n\")\n",
    "        print(\"SPACY: \",processed_sentence2,\"\\n\")\n",
    "        #if(len(processed_sentence1)>len(processed_sentence2)):\n",
    "        print(\"NLTK extra tokens: \",set(processed_sentence1)-set(processed_sentence2),\"\\n\")\n",
    "        #elif(len(processed_sentence1)<len(processed_sentence2)):\n",
    "        print(\"SPACY extra tokens:: \",set(processed_sentence2)-set(processed_sentence1),\"\\n\")\n",
    "#         else:\n",
    "#             print(\"Same Processing\",\"\\n\")\n",
    "        print('*'*100)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two results and explain which one is better and why?\n",
    "# Provide your answer here\n",
    "# According to me, preprocessing done by spacy is better than that done by nltk. \n",
    "# Preprocessing with nltk doesn't remove some unwanted tokens like digits or :\n",
    "# NLTK makes some sentences too short after preprocessing. For example, in sentence 21, \"information\" and \"long\" seem like important tokens.Spacy retains such tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
