{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3y8W_yvpTuj"
   },
   "source": [
    "#NLP Tutorial - PART I: Basic pre-processing and a very basic *chatbot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8AZqLtyEpOTh",
    "outputId": "385ba623-9797-4928-98e1-9e47d50b6b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR-v0UQXqE3i"
   },
   "source": [
    "Read in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mEjWukhappJy"
   },
   "outputs": [],
   "source": [
    "f=open('data.txt','r',errors = 'ignore')\n",
    "raw=f.readlines() #use line break to read in paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1kc6mcKp-0p",
    "outputId": "c38ae895-973b-4d9e-c048-cc939d865917"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A chatbot (also known as a spy, conversational bot, chatterbot, interactive agent, conversational interface, Conversational AI, talkbot or artificial spy entity) is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.',\n",
       " '[1] Such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the Turing test.',\n",
       " 'Chatbots are typically used in dialog systems for various practical purposes including customer service or information acquisition.',\n",
       " 'Some chatbots use sophisticated natural language processing systems, but many simpler ones scan for keywords within the input, then pull a reply with the most matching keywords, or the most similar wording pattern, from a database.',\n",
       " 'The term \"ChatterBot\" was originally coined by Michael Mauldin (creator of the first Verbot, Julia) in 1994 to describe these conversational programs.',\n",
       " \"[2] Today, most chatbots are accessed via virtual assistants such as Google Assistant and Amazon Alexa, via messaging apps such as Facebook Messenger or WeChat, or via individual organizations' apps and websites.\",\n",
       " '[3][4] Chatbots can be classified into usage categories such as conversational commerce (e-commerce via chat), analytics, communication, customer support, design, developer tools, education, entertainment, finance, food, games, health, HR, marketing, news, personal, productivity, shopping, social, sports, travel and utilities.',\n",
       " '[5]',\n",
       " 'Beyond chatbots, Conversational AI refers to the use of messaging apps, speech-based assistants and chatbots to automate communication and create personalized customer experiences at scale.',\n",
       " '[6]']"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents0 = [ nltk.sent_tokenize(r) for r in raw ] # converts each paragraph to a list of sentences \n",
    "sents0 = [ s for sent in sents0 for s in sent  ] # flatten the list\n",
    "print(len(sents0)) #how many sentences do we get?\n",
    "sents0[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWFf-IZmjxSE",
    "outputId": "012a4d4d-dacc-4025-ed8d-0f89ad7d3760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A chatbot (also known as a spy, conversational bot, chatterbot, interactive agent, conversational interface, Conversational AI, talkbot or artificial spy entity) is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.',\n",
       " '[1] Such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the Turing test.',\n",
       " 'Chatbots are typically used in dialog systems for various practical purposes including customer service or information acquisition.',\n",
       " 'Some chatbots use sophisticated natural language processing systems, but many simpler ones scan for keywords within the input, then pull a reply with the most matching keywords, or the most similar wording pattern, from a database.',\n",
       " 'The term \"ChatterBot\" was originally coined by Michael Mauldin (creator of the first Verbot, Julia) in 1994 to describe these conversational programs.',\n",
       " \"[2] Today, most chatbots are accessed via virtual assistants such as Google Assistant and Amazon Alexa, via messaging apps such as Facebook Messenger or WeChat, or via individual organizations' apps and websites.\",\n",
       " '[3][4] Chatbots can be classified into usage categories such as conversational commerce (e-commerce via chat), analytics, communication, customer support, design, developer tools, education, entertainment, finance, food, games, health, HR, marketing, news, personal, productivity, shopping, social, sports, travel and utilities.',\n",
       " 'Beyond chatbots, Conversational AI refers to the use of messaging apps, speech-based assistants and chatbots to automate communication and create personalized customer experiences at scale.',\n",
       " 'In 1950, Alan Turing\\'s famous article \"Computing Machinery and Intelligence\" was published,[7] which proposed what is now called the Turing test as a criterion of intelligence.',\n",
       " 'This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge, sufficiently well that the judge is unable to distinguish reliably—on the basis of the conversational content alone—between the program and a real human.']"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove short sentences\n",
    "sents = [ s for s in sents0 if len(nltk.word_tokenize(s)) > 8 ]\n",
    "print(len(sents))\n",
    "sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-iCxeFoqyVJ",
    "outputId": "0bef095c-1174-4e1c-d0ec-2d0cc16bdbd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chatbot',\n",
       " 'known',\n",
       " 'spi',\n",
       " 'convers',\n",
       " 'bot',\n",
       " 'chatterbot',\n",
       " 'interact',\n",
       " 'agent',\n",
       " 'convers',\n",
       " 'interfac',\n",
       " 'convers',\n",
       " 'ai',\n",
       " 'talkbot',\n",
       " 'artifici',\n",
       " 'spi',\n",
       " 'entiti',\n",
       " 'comput',\n",
       " 'program',\n",
       " 'artifici',\n",
       " 'intellig',\n",
       " 'conduct',\n",
       " 'convers',\n",
       " 'auditori',\n",
       " 'textual',\n",
       " 'method']"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare a preprocessing function that will do tokenization,\n",
    "# case lowering, punctuation removal, stopword removal, and stemming\n",
    "\n",
    "# create a stemmer\n",
    "snowball = nltk.SnowballStemmer('english')\n",
    "# get default stopword list\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "def MyNormalize(text):\n",
    "    tokens=nltk.word_tokenize(text.lower())\n",
    "    tokens=[ t for t in tokens if t not in string.punctuation ]\n",
    "    tokens=[ t for t in tokens if t not in my_stop_words ]\n",
    "    toks = [snowball.stem(t) for t in tokens  ]\n",
    "    return toks\n",
    "\n",
    "#test the preprocessing function\n",
    "MyNormalize(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8foQVwmq0dJ",
    "outputId": "15f1b4d7-021b-49c7-af8f-6fbca5601e50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 808)"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess the sentences in data, and create a tf-idf vector\n",
    "TfidfVec = TfidfVectorizer(tokenizer=MyNormalize)\n",
    "\n",
    "tfidf = TfidfVec.fit_transform(sents)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "vfMXPx5L4l-u"
   },
   "outputs": [],
   "source": [
    "#prepare some greeting words\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence): \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "        \n",
    "\n",
    "# function to match input to the preprocessed sentences\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    new = TfidfVec.transform([user_response])\n",
    "    vals = cosine_similarity(new[0], tfidf)\n",
    "    flat = vals.flatten()\n",
    "    idx = flat.argsort()[-1]\n",
    "    sim_max = flat[idx]\n",
    "    if(sim_max==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sents[idx]\n",
    "        return robo_response, sim_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FzHpTmS54tO2",
    "outputId": "a668e598-5551-4256-fed3-a6dd21d136b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Thus, for example, online help systems can usefully employ chatbot techniques to identify the area of help that users require, potentially providing a \"friendlier\" interface than a more formal search or menu system.',\n",
       " 0.28540638269572366)"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response(\"How can chatbots help people?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-JVCqQ8lax0",
    "outputId": "5fae05e1-ea50-4469-98d7-2a37134cc141"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chatbot', 'help', 'peopl']"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyNormalize(\"How can chatbots help people?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rb-2xYsy43XO",
    "outputId": "d3488a86-3cfb-4955-865a-5e9c933f8c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHATTY: My name is CHATTY. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
      "bye\n",
      "CHATTY: Bye! take care...\n"
     ]
    }
   ],
   "source": [
    "#starting the bot\n",
    "flag=True\n",
    "print(\"CHATTY: My name is CHATTY. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"CHATTY: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"CHATTY: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"CHATTY: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"CHATTY: Bye! take care...\")\n",
    "        \n",
    "# now chat with your bot...\n",
    "# you may experiment with different similarity functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enpeIfXgCqt8"
   },
   "source": [
    "# NLP Tutorial - PART II: NLU with spaCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "EryhiCZmCymT"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "#load the required model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#process a sentence\n",
    "eg1 = u\"What is the weather in Seattle today?\"\n",
    "doc1 = nlp(eg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "HdZx-__6C8lz",
    "outputId": "42c11b2e-2bf2-4cd4-a859-7d8ba9d484f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">What is the weather in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Seattle\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    today\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "?</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize the results for NER\n",
    "displacy.render(doc1, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "id": "DcoO-X3ADKqc",
    "outputId": "835bf71d-24ff-4640-d07d-710a835e652c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"9999e935a5964389b3bec5cedcb140ea-0\" class=\"displacy\" width=\"1275\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">What</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">weather</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Seattle</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">today?</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9999e935a5964389b3bec5cedcb140ea-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9999e935a5964389b3bec5cedcb140ea-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9999e935a5964389b3bec5cedcb140ea-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9999e935a5964389b3bec5cedcb140ea-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9999e935a5964389b3bec5cedcb140ea-0-2\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9999e935a5964389b3bec5cedcb140ea-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,266.5 L578.0,254.5 562.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9999e935a5964389b3bec5cedcb140ea-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9999e935a5964389b3bec5cedcb140ea-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9999e935a5964389b3bec5cedcb140ea-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9999e935a5964389b3bec5cedcb140ea-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9999e935a5964389b3bec5cedcb140ea-0-5\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 1100.0,2.0 1100.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9999e935a5964389b3bec5cedcb140ea-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,266.5 L1108.0,254.5 1092.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the results for dependency parsing\n",
    "displacy.render(doc1, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KQrZgZIDX2E",
    "outputId": "32b68c63-aa29-4963-b41e-789393fc111a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seattle 23 30 GPE\n",
      "today 31 36 DATE\n"
     ]
    }
   ],
   "source": [
    "# the detailed results behind \n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIBops2TDcWT",
    "outputId": "8f2abbac-4f09-4740-de5e-34fab66df9cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What what PRON WP attr is Xxxx True True\n",
      "is be AUX VBZ ROOT is xx True True\n",
      "the the DET DT det weather xxx True True\n",
      "weather weather NOUN NN nsubj is xxxx True False\n",
      "in in ADP IN prep weather xx True True\n",
      "Seattle Seattle PROPN NNP pobj in Xxxxx True False\n",
      "today today NOUN NN npadvmod is xxxx True False\n",
      "? ? PUNCT . punct is ? False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.head,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tshB_A-ZDncz",
    "outputId": "9ad39472-5290-4d03-dd06-d72584f7feb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3832489165356983\n",
      "0.7573910984233972\n",
      "0.7977538506573242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "eg2 = u\"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "eg3 = u\"What's the time now in Singapore?\"\n",
    "eg4 = u\"What's the weather now in Singapore?\"\n",
    "doc2 = nlp(eg2)\n",
    "doc3 = nlp(eg3)\n",
    "doc4 = nlp(eg4)\n",
    "print(doc2.similarity(doc1))\n",
    "print(doc3.similarity(doc1))\n",
    "print(doc4.similarity(doc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwlXPxb2bx5O",
    "outputId": "6394bb91-66a4-4a22-ced0-53797493caf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
      "\u001b[K     |████████████████████████████████| 96.4MB 1.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (56.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.10.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4.1)\n",
      "Building wheels for collected packages: en-core-web-md\n",
      "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp37-none-any.whl size=98051305 sha256=5ef670d84cc9611d32cb746f4e5336116889fae80befb1db06d1111bb0615366\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6_tw06fh/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
      "Successfully built en-core-web-md\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "#download the model with word vectors， which enables more accurate semantic similarity comparison\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "wwdj931Lc_En"
   },
   "outputs": [],
   "source": [
    "#load the model   \n",
    "import en_core_web_md\n",
    "nlpd = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "881Bx7LRDyJD",
    "outputId": "6b2d055c-ce10-4336-edb5-0dd29c379877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king True 7.1417456 False king PROPN NNP compound xxxx True False\n",
      "queen True 6.8297405 False queen PROPN NNP compound xxxx True False\n",
      "man True 6.352939 False man PROPN NNP compound xxx True False\n",
      "woman True 6.8987513 False woman NOUN NN ROOT xxxx True False\n",
      "king king 1.0\n",
      "king queen 0.72526103\n",
      "king man 0.4088461\n",
      "king woman 0.26556593\n",
      "queen king 0.72526103\n",
      "queen queen 1.0\n",
      "queen man 0.27109137\n",
      "queen woman 0.40660653\n",
      "man king 0.4088461\n",
      "man queen 0.27109137\n",
      "man man 1.0\n",
      "man woman 0.7401745\n",
      "woman king 0.26556593\n",
      "woman queen 0.40660653\n",
      "woman man 0.7401745\n",
      "woman woman 1.0\n"
     ]
    }
   ],
   "source": [
    "# common tokens come with vectors\n",
    "tokens = nlpd(u'king queen man woman')\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov, \n",
    "          token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n",
    "    \n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbGJUbnDdx9l",
    "outputId": "34a0d916-d37a-441f-e148-8588e5293866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6995620151208429\n",
      "0.9084785109142424\n",
      "0.9528884689826492\n"
     ]
    }
   ],
   "source": [
    "# more accurate similarity evaluation with the vectors available.\n",
    "doc1_md = nlpd(eg1)\n",
    "doc2_md = nlpd(eg2)\n",
    "doc3_md = nlpd(eg3)\n",
    "doc4_md = nlpd(eg4)\n",
    "print(doc2_md.similarity(doc1_md))\n",
    "print(doc3_md.similarity(doc1_md))\n",
    "print(doc4_md.similarity(doc1_md))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CGS-D2nlp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
