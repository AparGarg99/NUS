{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blqd8CdLR8Gd"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_89yP2qzEui",
    "outputId": "9c9c37a6-474d-4e1c-f745-14dc6a5ecec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.43.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.23.1)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: plot_keras_history in /usr/local/lib/python3.7/dist-packages (1.1.30)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (1.1.5)\n",
      "Requirement already satisfied: sanitize-ml-labels>=1.0.28 in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (1.0.29)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (1.4.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (3.2.2)\n",
      "Requirement already satisfied: compress-json in /usr/local/lib/python3.7/dist-packages (from sanitize-ml-labels>=1.0.28->plot_keras_history) (1.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (1.19.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (3.0.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->plot_keras_history) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->plot_keras_history) (2018.9)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.1.5)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install plot_keras_history\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QeiZ_Cqvzoqe",
    "outputId": "073b8724-d196-4e24-dc1d-402e43cdb9f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda, Reshape\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import dot\n",
    "from tensorflow.keras.activations import relu\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41VHqpiMSAGF"
   },
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DX4ENX1vVpqF"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Co69rL7jzrh3",
    "outputId": "51671359-c0cd-40b6-9068-c12094c06735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.'], ['statistical', 'hypothesis', 'testing', 'uses', 'a', 'null', 'hypothesis', ',', 'which', 'posits', 'randomness', '.'], ['hence', ',', 'when', 'we', 'look', 'at', 'linguistic', 'phenomena', 'in', 'corpora', ',', 'the', 'null', 'hypothesis', 'will', 'never', 'be', 'true', '.'], ['moreover', ',', 'where', 'there', 'is', 'enough', 'data', ',', 'we', 'shall', '(', 'almost', ')', 'always', 'be', 'able', 'to', 'establish', 'that', 'it', 'is', 'not', 'true', '.'], ['in', 'corpus', 'studies', ',', 'we', 'frequently', 'do', 'have', 'enough', 'data', ',', 'so', 'the', 'fact', 'that', 'a', 'relation', 'between', 'two', 'phenomena', 'is', 'demonstrably', 'non-random', ',', 'does', 'not', 'support', 'the', 'inference', 'that', 'it', 'is', 'not', 'arbitrary', '.'], ['we', 'present', 'experimental', 'evidence', 'of', 'how', 'arbitrary', 'associations', 'between', 'word', 'frequencies', 'and', 'corpora', 'are', 'systematically', 'non-random', '.'], ['we', 'review', 'literature', 'in', 'which', 'hypothesis', 'testing', 'has', 'been', 'used', ',', 'and', 'show', 'how', 'it', 'has', 'often', 'led', 'to', 'unhelpful', 'or', 'misleading', 'results', '.']]\n",
      "{0: ',', 1: '.', 2: 'and', 3: 'choose', 4: 'essentially', 5: 'is', 6: 'language', 7: 'never', 8: 'non-random', 9: 'randomly', 10: 'users', 11: 'words', 12: 'a', 13: 'hypothesis', 14: 'null', 15: 'posits', 16: 'randomness', 17: 'statistical', 18: 'testing', 19: 'uses', 20: 'which', 21: 'at', 22: 'be', 23: 'corpora', 24: 'hence', 25: 'in', 26: 'linguistic', 27: 'look', 28: 'phenomena', 29: 'the', 30: 'true', 31: 'we', 32: 'when', 33: 'will', 34: '(', 35: ')', 36: 'able', 37: 'almost', 38: 'always', 39: 'data', 40: 'enough', 41: 'establish', 42: 'it', 43: 'moreover', 44: 'not', 45: 'shall', 46: 'that', 47: 'there', 48: 'to', 49: 'where', 50: 'arbitrary', 51: 'between', 52: 'corpus', 53: 'demonstrably', 54: 'do', 55: 'does', 56: 'fact', 57: 'frequently', 58: 'have', 59: 'inference', 60: 'relation', 61: 'so', 62: 'studies', 63: 'support', 64: 'two', 65: 'are', 66: 'associations', 67: 'evidence', 68: 'experimental', 69: 'frequencies', 70: 'how', 71: 'of', 72: 'present', 73: 'systematically', 74: 'word', 75: 'been', 76: 'has', 77: 'led', 78: 'literature', 79: 'misleading', 80: 'often', 81: 'or', 82: 'results', 83: 'review', 84: 'show', 85: 'unhelpful', 86: 'used'}\n",
      "23\n",
      "and\n",
      "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]\n",
      "87\n",
      "Vocabulary Size: 88\n",
      "Vocabulary Sample: [(0, ','), (1, '.'), (2, 'and'), (3, 'choose'), (4, 'essentially'), (5, 'is'), (6, 'language'), (7, 'never'), (8, 'non-random'), (9, 'randomly')]\n"
     ]
    }
   ],
   "source": [
    "AlotOftext = \"\"\"Language users never choose words randomly, and language is essentially\n",
    "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
    "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
    "the null hypothesis will never be true. Moreover, where there is enough\n",
    "data, we shall (almost) always be able to establish that it is not true. In\n",
    "corpus studies, we frequently do have enough data, so the fact that a relation \n",
    "between two phenomena is demonstrably non-random, does not support the inference \n",
    "that it is not arbitrary. We present experimental evidence\n",
    "of how arbitrary associations between word frequencies and corpora are\n",
    "systematically non-random. We review literature in which hypothesis testing \n",
    "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize text\n",
    "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(AlotOftext)]\n",
    "print(tokenized_text)\n",
    "\n",
    "# Create Vocab as a Dictionary\n",
    "vocab = Dictionary(tokenized_text)\n",
    "print(dict(vocab.items()))\n",
    "\n",
    "print(vocab.token2id['corpora'])\n",
    "print(vocab[2])\n",
    "sent0 = tokenized_text[0]\n",
    "print(vocab.doc2idx(sent0))\n",
    "\n",
    "vocab.add_documents([['PAD']])\n",
    "dict(vocab.items())\n",
    "print(vocab.token2id['PAD'])\n",
    "\n",
    "corpusByWordID = list()\n",
    "for sent in  tokenized_text:\n",
    "    corpusByWordID.append(vocab.doc2idx(sent))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 100\n",
    "hidden_dim=100\n",
    "window_size = 2 # context window size\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBxqCqAL1BCo",
    "outputId": "440f8c42-bba2-46f6-9111-4113cd68f5dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['PAD', 'PAD', 'users', 'never'] -> Target (Y): language\n",
      "Context (X): ['PAD', 'language', 'never', 'choose'] -> Target (Y): users\n",
      "Context (X): ['language', 'users', 'choose', 'words'] -> Target (Y): never\n",
      "Context (X): ['users', 'never', 'words', 'randomly'] -> Target (Y): choose\n",
      "Context (X): ['never', 'choose', 'randomly', ','] -> Target (Y): words\n",
      "Context (X): ['choose', 'words', ',', 'and'] -> Target (Y): randomly\n",
      "Context (X): ['words', 'randomly', 'and', 'language'] -> Target (Y): ,\n",
      "Context (X): ['randomly', ',', 'language', 'is'] -> Target (Y): and\n",
      "Context (X): [',', 'and', 'is', 'essentially'] -> Target (Y): language\n",
      "Context (X): ['and', 'language', 'essentially', 'non-random'] -> Target (Y): is\n",
      "Context (X): ['language', 'is', 'non-random', '.'] -> Target (Y): essentially\n",
      "Context (X): ['is', 'essentially', '.', 'PAD'] -> Target (Y): non-random\n",
      "Context (X): ['essentially', 'non-random', 'PAD', 'PAD'] -> Target (Y): .\n",
      "Context (X): ['PAD', 'PAD', 'hypothesis', 'testing'] -> Target (Y): statistical\n",
      "Context (X): ['PAD', 'statistical', 'testing', 'uses'] -> Target (Y): hypothesis\n",
      "Context (X): ['statistical', 'hypothesis', 'uses', 'a'] -> Target (Y): testing\n",
      "Context (X): ['hypothesis', 'testing', 'a', 'null'] -> Target (Y): uses\n",
      "Context (X): ['testing', 'uses', 'null', 'hypothesis'] -> Target (Y): a\n",
      "Context (X): ['uses', 'a', 'hypothesis', ','] -> Target (Y): null\n",
      "Context (X): ['a', 'null', ',', 'which'] -> Target (Y): hypothesis\n",
      "Context (X): ['null', 'hypothesis', 'which', 'posits'] -> Target (Y): ,\n",
      "Context (X): ['hypothesis', ',', 'posits', 'randomness'] -> Target (Y): which\n",
      "Context (X): [',', 'which', 'randomness', '.'] -> Target (Y): posits\n",
      "Context (X): ['which', 'posits', '.', 'PAD'] -> Target (Y): randomness\n",
      "Context (X): ['posits', 'randomness', 'PAD', 'PAD'] -> Target (Y): .\n",
      "Context (X): ['PAD', 'PAD', ',', 'when'] -> Target (Y): hence\n",
      "Context (X): ['PAD', 'hence', 'when', 'we'] -> Target (Y): ,\n",
      "Context (X): ['hence', ',', 'we', 'look'] -> Target (Y): when\n",
      "Context (X): [',', 'when', 'look', 'at'] -> Target (Y): we\n",
      "Context (X): ['when', 'we', 'at', 'linguistic'] -> Target (Y): look\n",
      "Context (X): ['we', 'look', 'linguistic', 'phenomena'] -> Target (Y): at\n",
      "Context (X): ['look', 'at', 'phenomena', 'in'] -> Target (Y): linguistic\n",
      "Context (X): ['at', 'linguistic', 'in', 'corpora'] -> Target (Y): phenomena\n",
      "Context (X): ['linguistic', 'phenomena', 'corpora', ','] -> Target (Y): in\n",
      "Context (X): ['phenomena', 'in', ',', 'the'] -> Target (Y): corpora\n",
      "Context (X): ['in', 'corpora', 'the', 'null'] -> Target (Y): ,\n",
      "Context (X): ['corpora', ',', 'null', 'hypothesis'] -> Target (Y): the\n",
      "Context (X): [',', 'the', 'hypothesis', 'will'] -> Target (Y): null\n",
      "Context (X): ['the', 'null', 'will', 'never'] -> Target (Y): hypothesis\n",
      "Context (X): ['null', 'hypothesis', 'never', 'be'] -> Target (Y): will\n",
      "Context (X): ['hypothesis', 'will', 'be', 'true'] -> Target (Y): never\n",
      "Context (X): ['will', 'never', 'true', '.'] -> Target (Y): be\n",
      "Context (X): ['never', 'be', '.', 'PAD'] -> Target (Y): true\n",
      "Context (X): ['be', 'true', 'PAD', 'PAD'] -> Target (Y): .\n",
      "Context (X): ['PAD', 'PAD', ',', 'where'] -> Target (Y): moreover\n",
      "Context (X): ['PAD', 'moreover', 'where', 'there'] -> Target (Y): ,\n",
      "Context (X): ['moreover', ',', 'there', 'is'] -> Target (Y): where\n",
      "Context (X): [',', 'where', 'is', 'enough'] -> Target (Y): there\n",
      "Context (X): ['where', 'there', 'enough', 'data'] -> Target (Y): is\n",
      "Context (X): ['there', 'is', 'data', ','] -> Target (Y): enough\n",
      "Context (X): ['is', 'enough', ',', 'we'] -> Target (Y): data\n",
      "Context (X): ['enough', 'data', 'we', 'shall'] -> Target (Y): ,\n",
      "Context (X): ['data', ',', 'shall', '('] -> Target (Y): we\n",
      "Context (X): [',', 'we', '(', 'almost'] -> Target (Y): shall\n",
      "Context (X): ['we', 'shall', 'almost', ')'] -> Target (Y): (\n",
      "Context (X): ['shall', '(', ')', 'always'] -> Target (Y): almost\n",
      "Context (X): ['(', 'almost', 'always', 'be'] -> Target (Y): )\n",
      "Context (X): ['almost', ')', 'be', 'able'] -> Target (Y): always\n",
      "Context (X): [')', 'always', 'able', 'to'] -> Target (Y): be\n",
      "Context (X): ['always', 'be', 'to', 'establish'] -> Target (Y): able\n",
      "Context (X): ['be', 'able', 'establish', 'that'] -> Target (Y): to\n",
      "Context (X): ['able', 'to', 'that', 'it'] -> Target (Y): establish\n",
      "Context (X): ['to', 'establish', 'it', 'is'] -> Target (Y): that\n",
      "Context (X): ['establish', 'that', 'is', 'not'] -> Target (Y): it\n",
      "Context (X): ['that', 'it', 'not', 'true'] -> Target (Y): is\n",
      "Context (X): ['it', 'is', 'true', '.'] -> Target (Y): not\n",
      "Context (X): ['is', 'not', '.', 'PAD'] -> Target (Y): true\n",
      "Context (X): ['not', 'true', 'PAD', 'PAD'] -> Target (Y): .\n",
      "Context (X): ['PAD', 'PAD', 'corpus', 'studies'] -> Target (Y): in\n",
      "Context (X): ['PAD', 'in', 'studies', ','] -> Target (Y): corpus\n",
      "Context (X): ['in', 'corpus', ',', 'we'] -> Target (Y): studies\n",
      "Context (X): ['corpus', 'studies', 'we', 'frequently'] -> Target (Y): ,\n",
      "Context (X): ['studies', ',', 'frequently', 'do'] -> Target (Y): we\n",
      "Context (X): [',', 'we', 'do', 'have'] -> Target (Y): frequently\n",
      "Context (X): ['we', 'frequently', 'have', 'enough'] -> Target (Y): do\n",
      "Context (X): ['frequently', 'do', 'enough', 'data'] -> Target (Y): have\n",
      "Context (X): ['do', 'have', 'data', ','] -> Target (Y): enough\n",
      "Context (X): ['have', 'enough', ',', 'so'] -> Target (Y): data\n",
      "Context (X): ['enough', 'data', 'so', 'the'] -> Target (Y): ,\n",
      "Context (X): ['data', ',', 'the', 'fact'] -> Target (Y): so\n",
      "Context (X): [',', 'so', 'fact', 'that'] -> Target (Y): the\n",
      "Context (X): ['so', 'the', 'that', 'a'] -> Target (Y): fact\n",
      "Context (X): ['the', 'fact', 'a', 'relation'] -> Target (Y): that\n",
      "Context (X): ['fact', 'that', 'relation', 'between'] -> Target (Y): a\n",
      "Context (X): ['that', 'a', 'between', 'two'] -> Target (Y): relation\n",
      "Context (X): ['a', 'relation', 'two', 'phenomena'] -> Target (Y): between\n",
      "Context (X): ['relation', 'between', 'phenomena', 'is'] -> Target (Y): two\n",
      "Context (X): ['between', 'two', 'is', 'demonstrably'] -> Target (Y): phenomena\n",
      "Context (X): ['two', 'phenomena', 'demonstrably', 'non-random'] -> Target (Y): is\n",
      "Context (X): ['phenomena', 'is', 'non-random', ','] -> Target (Y): demonstrably\n",
      "Context (X): ['is', 'demonstrably', ',', 'does'] -> Target (Y): non-random\n",
      "Context (X): ['demonstrably', 'non-random', 'does', 'not'] -> Target (Y): ,\n",
      "Context (X): ['non-random', ',', 'not', 'support'] -> Target (Y): does\n",
      "Context (X): [',', 'does', 'support', 'the'] -> Target (Y): not\n",
      "Context (X): ['does', 'not', 'the', 'inference'] -> Target (Y): support\n",
      "Context (X): ['not', 'support', 'inference', 'that'] -> Target (Y): the\n",
      "Context (X): ['support', 'the', 'that', 'it'] -> Target (Y): inference\n",
      "Context (X): ['the', 'inference', 'it', 'is'] -> Target (Y): that\n",
      "Context (X): ['inference', 'that', 'is', 'not'] -> Target (Y): it\n",
      "Context (X): ['that', 'it', 'not', 'arbitrary'] -> Target (Y): is\n",
      "Context (X): ['it', 'is', 'arbitrary', '.'] -> Target (Y): not\n",
      "Context (X): ['is', 'not', '.', 'PAD'] -> Target (Y): arbitrary\n",
      "Context (X): ['not', 'arbitrary', 'PAD', 'PAD'] -> Target (Y): .\n",
      "Context (X): ['PAD', 'PAD', 'present', 'experimental'] -> Target (Y): we\n",
      "Context (X): ['PAD', 'we', 'experimental', 'evidence'] -> Target (Y): present\n",
      "Context (X): ['we', 'present', 'evidence', 'of'] -> Target (Y): experimental\n",
      "Context (X): ['present', 'experimental', 'of', 'how'] -> Target (Y): evidence\n",
      "Context (X): ['experimental', 'evidence', 'how', 'arbitrary'] -> Target (Y): of\n",
      "Context (X): ['evidence', 'of', 'arbitrary', 'associations'] -> Target (Y): how\n",
      "Context (X): ['of', 'how', 'associations', 'between'] -> Target (Y): arbitrary\n",
      "Context (X): ['how', 'arbitrary', 'between', 'word'] -> Target (Y): associations\n",
      "Context (X): ['arbitrary', 'associations', 'word', 'frequencies'] -> Target (Y): between\n",
      "Context (X): ['associations', 'between', 'frequencies', 'and'] -> Target (Y): word\n",
      "Context (X): ['between', 'word', 'and', 'corpora'] -> Target (Y): frequencies\n",
      "Context (X): ['word', 'frequencies', 'corpora', 'are'] -> Target (Y): and\n",
      "Context (X): ['frequencies', 'and', 'are', 'systematically'] -> Target (Y): corpora\n",
      "Context (X): ['and', 'corpora', 'systematically', 'non-random'] -> Target (Y): are\n",
      "Context (X): ['corpora', 'are', 'non-random', '.'] -> Target (Y): systematically\n",
      "Context (X): ['are', 'systematically', '.', 'PAD'] -> Target (Y): non-random\n",
      "Context (X): ['systematically', 'non-random', 'PAD', 'PAD'] -> Target (Y): .\n",
      "Context (X): ['PAD', 'PAD', 'review', 'literature'] -> Target (Y): we\n",
      "Context (X): ['PAD', 'we', 'literature', 'in'] -> Target (Y): review\n",
      "Context (X): ['we', 'review', 'in', 'which'] -> Target (Y): literature\n",
      "Context (X): ['review', 'literature', 'which', 'hypothesis'] -> Target (Y): in\n",
      "Context (X): ['literature', 'in', 'hypothesis', 'testing'] -> Target (Y): which\n",
      "Context (X): ['in', 'which', 'testing', 'has'] -> Target (Y): hypothesis\n",
      "Context (X): ['which', 'hypothesis', 'has', 'been'] -> Target (Y): testing\n",
      "Context (X): ['hypothesis', 'testing', 'been', 'used'] -> Target (Y): has\n",
      "Context (X): ['testing', 'has', 'used', ','] -> Target (Y): been\n",
      "Context (X): ['has', 'been', ',', 'and'] -> Target (Y): used\n",
      "Context (X): ['been', 'used', 'and', 'show'] -> Target (Y): ,\n",
      "Context (X): ['used', ',', 'show', 'how'] -> Target (Y): and\n",
      "Context (X): [',', 'and', 'how', 'it'] -> Target (Y): show\n",
      "Context (X): ['and', 'show', 'it', 'has'] -> Target (Y): how\n",
      "Context (X): ['show', 'how', 'has', 'often'] -> Target (Y): it\n",
      "Context (X): ['how', 'it', 'often', 'led'] -> Target (Y): has\n",
      "Context (X): ['it', 'has', 'led', 'to'] -> Target (Y): often\n",
      "Context (X): ['has', 'often', 'to', 'unhelpful'] -> Target (Y): led\n",
      "Context (X): ['often', 'led', 'unhelpful', 'or'] -> Target (Y): to\n",
      "Context (X): ['led', 'to', 'or', 'misleading'] -> Target (Y): unhelpful\n",
      "Context (X): ['to', 'unhelpful', 'misleading', 'results'] -> Target (Y): or\n",
      "Context (X): ['unhelpful', 'or', 'results', '.'] -> Target (Y): misleading\n",
      "Context (X): ['or', 'misleading', '.', 'PAD'] -> Target (Y): results\n",
      "Context (X): ['misleading', 'results', 'PAD', 'PAD'] -> Target (Y): .\n"
     ]
    }
   ],
   "source": [
    "# Create CBOW Training data\n",
    "def generate_cbow_context_word_pairs(corpusByID, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for sent in corpusByID:\n",
    "        sentence_length = len(sent)\n",
    "        for index, word in enumerate(sent):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([sent[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "            if start<0:\n",
    "                x = sequence.pad_sequences(context_words, maxlen=context_length,padding='pre',value=vocab.token2id['PAD'])\n",
    "                y = np_utils.to_categorical(label_word, vocab_size)\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "                continue\n",
    "            if end>=sentence_length:\n",
    "                x = sequence.pad_sequences(context_words, maxlen=context_length,padding='post',value=vocab.token2id['PAD'])\n",
    "                y = np_utils.to_categorical(label_word, vocab_size)\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "                continue\n",
    "            else:\n",
    "                X.append(sequence.pad_sequences(context_words, maxlen=context_length))\n",
    "                y = np_utils.to_categorical(label_word, vocab_size)\n",
    "                Y.append(y)\n",
    "                continue\n",
    "           \n",
    "    return X,Y\n",
    "            \n",
    "# Test this out for some samples\n",
    "\n",
    "\n",
    "X,Y = generate_cbow_context_word_pairs(corpusByWordID, window_size, vocab_size) \n",
    "   \n",
    "for x, y in zip(X,Y):\n",
    "    print('Context (X):', [vocab[w] for w in x[0]], '-> Target (Y):', vocab[np.argwhere(y[0])[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lVgUJ6jSOGN"
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "LAgiFVl5QBs4"
   },
   "outputs": [],
   "source": [
    "V = len(vocab) \n",
    "N = 100 \n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JH_iy4Lh1JUa",
    "outputId": "1f39f8a3-d88c-453a-ead4-9e6e02d945c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 4, 100)            8800      \n",
      "                                                                 \n",
      " lambda_4 (Lambda)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 88)                8888      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,688\n",
      "Trainable params: 17,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cbow = Sequential()\n",
    "\n",
    "cbow.add(Embedding(input_dim = V, \n",
    "                   output_dim = N,\n",
    "                   input_length = window_size*2)) \n",
    "\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(N,1)))\n",
    "\n",
    "cbow.add(Dense(V, activation='relu'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuVqi7WGDxNe",
    "outputId": "eec9c132-4415-4362-db1a-192544098689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1567.5061424970627\n",
      "1 1269.7155812978745\n",
      "2 1194.2798880338669\n",
      "3 1178.5544596910477\n",
      "4 1171.8559724092484\n",
      "5 1166.545238852501\n",
      "6 1162.1016528606415\n",
      "7 1158.2608840465546\n",
      "8 1154.8661383390427\n",
      "9 1151.8166155815125\n",
      "10 1149.0421355962753\n",
      "11 1146.4915161132812\n",
      "12 1144.127556681633\n",
      "13 1141.9210551977158\n",
      "14 1139.849433541298\n",
      "15 1137.8944646120071\n",
      "16 1136.0420526266098\n",
      "17 1134.279482960701\n",
      "18 1132.596445798874\n",
      "19 1130.9848074913025\n",
      "20 1129.4368628263474\n",
      "21 1127.9470437765121\n",
      "22 1126.5100889205933\n",
      "23 1125.1210478544235\n",
      "24 1123.7765136957169\n",
      "25 1122.4734536409378\n",
      "26 1121.2085974216461\n",
      "27 1119.9788310527802\n",
      "28 1118.7821642160416\n",
      "29 1117.6156251430511\n",
      "30 1116.4775099754333\n",
      "31 1115.3656096458435\n",
      "32 1114.278504371643\n",
      "33 1113.2147631645203\n",
      "34 1112.1723136901855\n",
      "35 1111.1506071090698\n",
      "36 1110.148718714714\n",
      "37 1109.1651582717896\n",
      "38 1108.198615193367\n",
      "39 1107.2480803728104\n",
      "40 1106.3123837709427\n",
      "41 1105.3907358646393\n",
      "42 1104.482270359993\n",
      "43 1103.5867277383804\n",
      "44 1102.70365858078\n",
      "45 1101.8323880434036\n",
      "46 1100.9721405506134\n",
      "47 1100.122222185135\n",
      "48 1099.2825589179993\n",
      "49 1098.4525328874588\n",
      "50 1097.6321110725403\n",
      "51 1096.8213902711868\n",
      "52 1096.0210356712341\n",
      "53 1095.2337379455566\n",
      "54 1094.4545093774796\n",
      "55 1093.6867977380753\n",
      "56 1092.9367402791977\n",
      "57 1092.20325922966\n",
      "58 1091.4866503477097\n",
      "59 1090.7780593633652\n",
      "60 1090.0780514478683\n",
      "61 1089.38510119915\n",
      "62 1088.6994709968567\n",
      "63 1088.0200134515762\n",
      "64 1087.347185254097\n",
      "65 1086.6841143369675\n",
      "66 1086.02785551548\n",
      "67 1085.3776693344116\n",
      "68 1084.7391200065613\n",
      "69 1084.1071977615356\n",
      "70 1083.4803385734558\n",
      "71 1082.8614916801453\n",
      "72 1082.2492980957031\n",
      "73 1081.6418906450272\n",
      "74 1081.0401139259338\n",
      "75 1080.4450850486755\n",
      "76 1079.8557879924774\n",
      "77 1079.2750552892685\n",
      "78 1078.7005447149277\n",
      "79 1078.13143324852\n",
      "80 1077.5667222738266\n",
      "81 1077.005479335785\n",
      "82 1076.4476158618927\n",
      "83 1075.8943440914154\n",
      "84 1075.3459559679031\n",
      "85 1074.8010597229004\n",
      "86 1074.259246468544\n",
      "87 1073.7208641767502\n",
      "88 1073.187683224678\n",
      "89 1072.658279299736\n",
      "90 1072.1326870918274\n",
      "91 1071.61063683033\n",
      "92 1071.091718196869\n",
      "93 1070.576416015625\n",
      "94 1070.0658733844757\n",
      "95 1069.5610411167145\n",
      "96 1069.0608489513397\n",
      "97 1068.564797759056\n",
      "98 1068.0732295513153\n",
      "99 1067.5843687057495\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "\n",
    "for epoch in range(100):\n",
    "    loss = 0.\n",
    "    for x, y in zip(X,Y):\n",
    "      loss += cbow.train_on_batch(x, y)\n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "dxPVT_G-1RYz"
   },
   "outputs": [],
   "source": [
    "## Save the wordvectors\n",
    "f = open('Cbow_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = cbow.get_weights()[0]\n",
    "for key in vocab:\n",
    "    str_vec = ' '.join(map(str, list(vectors[key, :])))\n",
    "    f.write('{} {}\\n'.format(vocab[key], str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1H3zTpE1Uwr",
    "outputId": "64caab54-7810-468a-f302-1f96682bfcda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('non-random', 0.7850772738456726),\n",
       " ('and', 0.7158022522926331),\n",
       " ('essentially', 0.7134977579116821),\n",
       " ('choose', 0.6051605939865112),\n",
       " ('words', 0.5714943408966064),\n",
       " ('users', 0.5539845824241638),\n",
       " ('not', 0.43584901094436646),\n",
       " ('that', 0.4232177436351776),\n",
       " ('demonstrably', 0.4219173192977905),\n",
       " ('systematically', 0.41441741585731506)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the vectors back and validate\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./Cbow_vectors.txt', binary=False)\n",
    "\n",
    "w2v.most_similar(positive=['language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FrnbCrrSR0W"
   },
   "source": [
    "# Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aF_X1GAsVxll"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4z88oUR1Yk5",
    "outputId": "f1d10fc6-142c-44ef-a70a-fa822ac5a879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(choose (3), and (2)) -> 0\n",
      "(. (1), non-random (8)) -> 1\n",
      "(language (6), never (7)) -> 1\n",
      "(never (7), been (75)) -> 0\n",
      "(never (7), words (11)) -> 1\n",
      "(language (6), is (5)) -> 1\n",
      "(and (2), or (81)) -> 0\n",
      "(non-random (8), . (1)) -> 1\n",
      "(is (5), essentially (4)) -> 1\n",
      "(. (1), essentially (4)) -> 1\n"
     ]
    }
   ],
   "source": [
    "# generate skip-grams with both positive and negative examples\n",
    "skip_grams = [skipgrams(sent, vocabulary_size=vocab_size, window_size=2) for sent in corpusByWordID]\n",
    "\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "        vocab[pairs[i][0]], pairs[i][0],           \n",
    "        vocab[pairs[i][1]], pairs[i][1], \n",
    "        labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qV8P1WiSYYP"
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "P_qwMpDhRKFG"
   },
   "outputs": [],
   "source": [
    "V = len(vocab) \n",
    "N = 100 \n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EqD40Iq11fpg",
    "outputId": "56dd7613-ce89-4f3d-89bb-2f100c09acc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)     (None, 1, 100)       8800        ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " context_embedding (Embedding)  (None, 1, 100)       8800        ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 100, 1)       0           ['word_embedding[0][0]']         \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 100, 1)       0           ['context_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 1, 1)         0           ['reshape_3[0][0]',              \n",
      "                                                                  'reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 1)            0           ['dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            2           ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,602\n",
      "Trainable params: 17,602\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#define the skip-gram model\n",
    "\n",
    "input_word = Input((1,))\n",
    "input_context_word = Input((1,))\n",
    "\n",
    "word_embedding    = Embedding(input_dim=V, output_dim=N,input_length=1,name='word_embedding')\n",
    "context_embedding = Embedding(input_dim=V, output_dim=N,input_length=1,name='context_embedding')\n",
    "\n",
    "word_embedding = word_embedding(input_word)\n",
    "word_embedding_layer = Reshape((N, 1))(word_embedding)\n",
    "\n",
    "context_embedding = context_embedding(input_context_word)\n",
    "context_embedding_layer = Reshape((N, 1))(context_embedding)\n",
    "\n",
    "# now perform the dot product operation  \n",
    "dot_product = dot([word_embedding_layer, context_embedding_layer], axes=1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "# add the sigmoid output layer\n",
    "outputLayer = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "model = Model(inputs=[input_word, input_context_word], outputs=outputLayer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# view model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVpBqKfo1iSj",
    "outputId": "e420bd7c-5fc3-48e8-c6d6-c2a08384823c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 1 Loss: 4.852176606655121\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 2 Loss: 4.844983518123627\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 3 Loss: 4.838289499282837\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 4 Loss: 4.831020832061768\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 5 Loss: 4.82278436422348\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 6 Loss: 4.813201487064362\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 7 Loss: 4.8018800020217896\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 8 Loss: 4.788412928581238\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 9 Loss: 4.772378742694855\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 10 Loss: 4.753344416618347\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 11 Loss: 4.730868995189667\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 12 Loss: 4.704509198665619\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 13 Loss: 4.673826575279236\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 14 Loss: 4.638397634029388\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 15 Loss: 4.5978222489356995\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 16 Loss: 4.551736533641815\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 17 Loss: 4.499821782112122\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 18 Loss: 4.441819429397583\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 19 Loss: 4.377539396286011\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 20 Loss: 4.306873321533203\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 21 Loss: 4.229802191257477\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 22 Loss: 4.14640486240387\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 23 Loss: 4.056862950325012\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 24 Loss: 3.9614638686180115\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 25 Loss: 3.8606009483337402\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 26 Loss: 3.754767656326294\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 27 Loss: 3.6445511281490326\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 28 Loss: 3.53061980009079\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 29 Loss: 3.4137068390846252\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 30 Loss: 3.294594168663025\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 31 Loss: 3.174089878797531\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 32 Loss: 3.0530075132846832\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 33 Loss: 2.9321442544460297\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 34 Loss: 2.8122618794441223\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 35 Loss: 2.6940678358078003\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 36 Loss: 2.578201949596405\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 37 Loss: 2.465224802494049\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 38 Loss: 2.3556123673915863\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 39 Loss: 2.249753162264824\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 40 Loss: 2.1479488909244537\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 41 Loss: 2.0504188239574432\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 42 Loss: 1.9573057442903519\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 43 Loss: 1.868684396147728\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 44 Loss: 1.784568876028061\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 45 Loss: 1.7049231231212616\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 46 Loss: 1.6296685338020325\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 47 Loss: 1.5586929470300674\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 48 Loss: 1.4918583035469055\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 49 Loss: 1.42900650203228\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 50 Loss: 1.369966372847557\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 51 Loss: 1.3145577311515808\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 52 Loss: 1.2625959441065788\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 53 Loss: 1.2138949260115623\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 54 Loss: 1.1682702079415321\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 55 Loss: 1.1255405396223068\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 56 Loss: 1.0855303183197975\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 57 Loss: 1.0480693206191063\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 58 Loss: 1.0129953175783157\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 59 Loss: 0.9801531732082367\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 60 Loss: 0.9493958428502083\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 61 Loss: 0.9205843731760979\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 62 Loss: 0.89358801394701\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 63 Loss: 0.8682838082313538\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 64 Loss: 0.8445564359426498\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 65 Loss: 0.8222983777523041\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 66 Loss: 0.801409088075161\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 67 Loss: 0.7817946597933769\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 68 Loss: 0.7633679509162903\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 69 Loss: 0.7460475377738476\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 70 Loss: 0.7297579571604729\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 71 Loss: 0.7144290395081043\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 72 Loss: 0.6999956592917442\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 73 Loss: 0.686397098004818\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 74 Loss: 0.6735773496329784\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 75 Loss: 0.6614843234419823\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 76 Loss: 0.6500696577131748\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 77 Loss: 0.6392883248627186\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 78 Loss: 0.6290988437831402\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 79 Loss: 0.6194625571370125\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 80 Loss: 0.6103435344994068\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 81 Loss: 0.6017085239291191\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 82 Loss: 0.59352657943964\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 83 Loss: 0.5857689790427685\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 84 Loss: 0.5784091874957085\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 85 Loss: 0.5714221149682999\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 86 Loss: 0.5647848397493362\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 87 Loss: 0.5584759786725044\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 88 Loss: 0.5524755865335464\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 89 Loss: 0.5467649362981319\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 90 Loss: 0.5413268506526947\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 91 Loss: 0.5361451283097267\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 92 Loss: 0.5312048383057117\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 93 Loss: 0.5264919400215149\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 94 Loss: 0.5219932869076729\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 95 Loss: 0.5176967345178127\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 96 Loss: 0.513590894639492\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 97 Loss: 0.5096651054918766\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 98 Loss: 0.5059093497693539\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 99 Loss: 0.5023144446313381\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    loss = 0\n",
    "    for i, elem in enumerate(skip_grams):\n",
    "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [pair_first_elem, pair_second_elem]\n",
    "        Y = labels\n",
    "        if i % 10000 == 0:\n",
    "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "\n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "CjhkbLsp1k0Y"
   },
   "outputs": [],
   "source": [
    "#get the embeding matrix\n",
    "weights = model.get_weights()\n",
    "## Save the wordvectors\n",
    "f = open('skipgram_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "for key in vocab:\n",
    "    str_vec = ' '.join(map(str, list(vectors[key, :])))\n",
    "    f.write('{} {}\\n'.format(vocab[key], str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4rUPCJC1mvc",
    "outputId": "c3ad61a9-3933-49aa-830e-722a152a5fc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('has', 0.5080244541168213),\n",
       " ('will', 0.39930760860443115),\n",
       " ('there', 0.38471418619155884),\n",
       " ('inference', 0.37814533710479736),\n",
       " ('show', 0.3536876142024994),\n",
       " ('to', 0.2860047519207001),\n",
       " ('used', 0.27465033531188965),\n",
       " ('testing', 0.27205532789230347),\n",
       " ('often', 0.24003587663173676),\n",
       " ('statistical', 0.1933324635028839)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the vectors back and validate\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./skipgram_vectors.txt', binary=False)\n",
    "w2v.most_similar(positive=['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Vs8ng_Zf1o08"
   },
   "outputs": [],
   "source": [
    "#Excerise: \n",
    "#modeify the skipegram_model to share the same embeding layer between word and context\n",
    "#Discussion: which is better? Why?  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Word2Vec_workshop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
