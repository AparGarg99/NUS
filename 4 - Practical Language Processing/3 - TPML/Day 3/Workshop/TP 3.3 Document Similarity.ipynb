{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"nlp","language":"python","name":"nlp"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"TP 3.3 Document Similarity.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"5M_yMUWzAK-n"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qMTL5vkmAK-o"},"source":["# Document similarity methods\n","\n","In this notebook, we illustrate different document similarity methods and use them to retrieve similar customer reviews. The method word movers' distance is illustrated. This method depends on word embeddings too, but also look at an 'individual words' viewpoint.\n","\n","## Word movers' distance\n","\n","WMD uses normalised word embeddings and Bag of words to calculate distance between sentences/ documents. It resolves the problem of synonyms between sentences. The intution behind the method is that we find the minimum \"traveling distance\" between documents, in other words the most efficient way to \"move\" the distribution of sentence/document 1 to the distribution of sentence/ document 2.\n","\n","A good illustration is the two sentences below:\n","sentence_obama and sentence_president"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BxZOoFDOEmEs","executionInfo":{"status":"ok","timestamp":1610525474898,"user_tz":-480,"elapsed":23504,"user":{"displayName":"Aobo Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1R4n3TLad19eGDFJcbKQpak37bv0BcjEpPoimgw=s64","userId":"18082252328038546601"}},"outputId":"3de71bd1-d158-4ad0-b588-619d150407c0"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fnKEO_eDAK-o"},"source":["# Image from https://vene.ro/images/wmd-obama.png\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from time import time\n","\n","#img = mpimg.imread('/content/drive/MyDrive/Colab Notebooks/images/wmd-obama.png')\n","#imgplot = plt.imshow(img)\n","#plt.axis('off')\n","#plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nn7pzHKmAK-o"},"source":["# Initialize logging.\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","sentence_obama = 'Obama speaks to the media in Illinois'\n","sentence_president = 'The president greets the press in Chicago'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"okeNZbykAK-o"},"source":["## Preprocessing\n","First, let's do some pre-processing, removing stopwords and punctuation. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5CoWTmvAK-o","executionInfo":{"status":"ok","timestamp":1610525712629,"user_tz":-480,"elapsed":911,"user":{"displayName":"Aobo Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1R4n3TLad19eGDFJcbKQpak37bv0BcjEpPoimgw=s64","userId":"18082252328038546601"}},"outputId":"c690d3bc-09ed-4d60-914b-1efdf4a1527c"},"source":["# Import and download stopwords from NLTK.\n","from nltk.corpus import stopwords\n","from nltk import download\n","download('stopwords')  # Download stopwords list.\n","stop_words = stopwords.words('english')\n","\n","# Pre-processing a document.\n","from nltk import word_tokenize\n","download('punkt')  # Download data for tokenizer.\n","\n","def preprocess(doc):\n","    doc = doc.lower()  # Lower the text.\n","    doc = word_tokenize(doc)  # Split into words.\n","    doc = [w for w in doc if not w in stop_words]  # Remove stopwords.\n","    doc = [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n","    return doc\n","\n","sentence_obama = preprocess(sentence_obama)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CD4yYlwJAK-o"},"source":["We will use the word vectors that are pre-trained and available from google. https://code.google.com/archive/p/word2vec/ They are hosted in gensim. These are from part of Google News dataset (about 100 billion words) with 300-dimensional vectors for 3 million words and phrases.\n","\n","More options:\n","https://radimrehurek.com/gensim/models/word2vec.html"]},{"cell_type":"code","metadata":{"id":"uDaKcqHXAK-o"},"source":["import gensim.downloader as api\n","from gensim.models import Word2Vec\n","model = api.load('word2vec-google-news-300')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"82ecs-doAK-p"},"source":["The WMD model is based on the word vectors, and can be obtained directly from the distance measure from the individual word vectors. Note the sentence on Obama and the president has a distance of 3.79. \n","Note that similarity and distance are opposites. When the distance is high, the similarity is low. Vice versa is true as well."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8O8e53-IAK-p","executionInfo":{"status":"ok","timestamp":1610526066485,"user_tz":-480,"elapsed":824,"user":{"displayName":"Aobo Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1R4n3TLad19eGDFJcbKQpak37bv0BcjEpPoimgw=s64","userId":"18082252328038546601"}},"outputId":"5cd260a4-9487-4aeb-cef8-b5422800085b"},"source":["distance = model.wmdistance(sentence_obama, sentence_president)\n","print('distance = %.4f' % distance)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-01-13 08:21:06,038 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n","2021-01-13 08:21:06,039 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n","2021-01-13 08:21:06,040 : INFO : built Dictionary(18 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'C']...) from 2 documents (total 38 corpus positions)\n"],"name":"stderr"},{"output_type":"stream","text":["distance = 3.7959\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rZoVetx4AK-p"},"source":["Notice that in another case with a very different sentence talking about oranges, the distance is bigger (less similar) at 4.4"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZeVWgneAK-p","executionInfo":{"status":"ok","timestamp":1610526069307,"user_tz":-480,"elapsed":826,"user":{"displayName":"Aobo Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1R4n3TLad19eGDFJcbKQpak37bv0BcjEpPoimgw=s64","userId":"18082252328038546601"}},"outputId":"3845eaa7-dd40-4f35-caca-b79dbfda7814"},"source":["sentence_orange = preprocess('Oranges are my favorite fruit')\n","distance = model.wmdistance(sentence_obama, sentence_orange)\n","print('distance = %.4f' % distance)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-01-13 08:21:08,857 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n","2021-01-13 08:21:08,859 : INFO : built Dictionary(7 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'favorite']...) from 2 documents (total 7 corpus positions)\n"],"name":"stderr"},{"output_type":"stream","text":["distance = 4.3802\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jYQ2m-fsPRKe"},"source":["## Normalising WMD\n"," \n","In the WMD distance, sentences of different lenghts can increase the distance. This is due to different word vectors lengths of the sentences. To mitigate this, the document vectors of the sentences are each normalised to the same dimension. The normalisation in this case reduces the distance lengths."]},{"cell_type":"code","metadata":{"id":"Wp8HHu60AK-p"},"source":["model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n","\n","distance = model.wmdistance(sentence_obama, sentence_president)  # Compute WMD as normal.\n","print('distance: %r' % distance)\n","\n","distance = model.wmdistance(sentence_obama, sentence_orange)\n","print('distance = %.4f' % distance)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K4QPo2TkR1lu"},"source":["## Building our own word vectors\n","\n","Instead of using the word vectors from Google, we train our own word vectors in this yelp reviews case. This is in json format and in particular we look at 6 restaurant IDs. This reviews dataset is available from Kaggle. https://www.kaggle.com/yelp-dataset/yelp-dataset \n","The following code reads in the reviews one by one into the wmd_corpus (only for the 6 restaurant IDs) and the w2v corpus (for training word vectors.)\n","\n","As always, training a new word vector model is advantageous since this trained model will reflect the semantics of this reviews case. "]},{"cell_type":"code","metadata":{"id":"6bFREs3sAK-p"},"source":["start = time()\n","\n","import json\n","\n","# Review IDs of the restaurants.\n","ids = ['fWKvX83p0-ka4JS3dc6E5A', 'IjZ33sJrzXqU-0X6U8NwyA', 'IESLBzqUCLdSzSqm0eCSxQ',\n","      '1uJFq2r5QfJG_6ExMRCaGw', 'm2CKSsepBCoRYWxiRUsxAg', 'jJAIXA46pU1swYyRCdfXtQ']\n","\n","w2v_corpus = []  # Documents to train word2vec on (all 6 restaurants).\n","wmd_corpus = []  # Documents to run queries against (only one restaurant).\n","documents = []  # wmd_corpus, with no pre-processing (so we can see the original documents).\n","with open('/content/drive/MyDrive/Colab Data/yelp_academic_dataset_review.json') as data_file: \n","    for line in data_file:\n","        json_line = json.loads(line)\n","        if json_line['review_id'] not in ids:\n","            # Not one of the 6 restaurants.\n","            continue\n","         \n","        # Pre-process document.\n","        text = json_line['text']  # Extract text from JSON object.\n","        text = preprocess(text)\n","        # Add to corpus for training Word2Vec.\n","        w2v_corpus.append(text)\n","\n","        if json_line['review_id'] in ids:\n","            # Add to corpus for similarity queries.\n","            wmd_corpus.append(text)\n","            documents.append(json_line['text'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E3-hPHhHa12V"},"source":["Train a new word vector model for the WMD similarity and then use it for the Word similarity model (WMD). "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OZrtx4J2AK-p","executionInfo":{"status":"ok","timestamp":1610526209111,"user_tz":-480,"elapsed":24036,"user":{"displayName":"Aobo Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1R4n3TLad19eGDFJcbKQpak37bv0BcjEpPoimgw=s64","userId":"18082252328038546601"}},"outputId":"6e764cd7-6252-4031-a069-8c450e322904"},"source":["# Train Word2Vec on all the restaurants. \n","n_model = Word2Vec(w2v_corpus, workers=3, size=100, min_count=1)\n","\n","# Initialize WmdSimilarity.\n","from gensim.similarities import WmdSimilarity\n","num_best = 5\n","instance = WmdSimilarity(wmd_corpus, model, num_best=5)  # means the top 5 documents are retrieved\n","# possible sentences come from only the wmd_corpus.\n","# WMD based on the word2vec from w2c corpus.\n","# you must first build vocabulary before training the model --> usually due to empty w2v_corpus"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-01-13 08:23:05,471 : INFO : collecting all words and their counts\n","2021-01-13 08:23:05,472 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","2021-01-13 08:23:05,474 : INFO : collected 318 word types from a corpus of 438 raw words and 6 sentences\n","2021-01-13 08:23:05,476 : INFO : Loading a fresh vocabulary\n","2021-01-13 08:23:05,478 : INFO : effective_min_count=1 retains 318 unique words (100% of original 318, drops 0)\n","2021-01-13 08:23:05,479 : INFO : effective_min_count=1 leaves 438 word corpus (100% of original 438, drops 0)\n","2021-01-13 08:23:05,481 : INFO : deleting the raw counts dictionary of 318 items\n","2021-01-13 08:23:05,482 : INFO : sample=0.001 downsamples 73 most-common words\n","2021-01-13 08:23:05,483 : INFO : downsampling leaves estimated 354 word corpus (80.9% of prior 438)\n","2021-01-13 08:23:05,489 : INFO : estimated required memory for 318 words and 100 dimensions: 413400 bytes\n","2021-01-13 08:23:05,491 : INFO : resetting layer weights\n","2021-01-13 08:23:05,561 : INFO : training model with 3 workers on 318 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n","2021-01-13 08:23:05,565 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-01-13 08:23:05,565 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-01-13 08:23:05,567 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-01-13 08:23:05,567 : INFO : EPOCH - 1 : training on 438 raw words (359 effective words) took 0.0s, 114834 effective words/s\n","2021-01-13 08:23:05,574 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-01-13 08:23:05,575 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-01-13 08:23:05,575 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-01-13 08:23:05,577 : INFO : EPOCH - 2 : training on 438 raw words (354 effective words) took 0.0s, 100280 effective words/s\n","2021-01-13 08:23:05,584 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-01-13 08:23:05,585 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-01-13 08:23:05,587 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-01-13 08:23:05,587 : INFO : EPOCH - 3 : training on 438 raw words (354 effective words) took 0.0s, 95969 effective words/s\n","2021-01-13 08:23:05,593 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-01-13 08:23:05,594 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-01-13 08:23:05,594 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-01-13 08:23:05,596 : INFO : EPOCH - 4 : training on 438 raw words (359 effective words) took 0.0s, 109260 effective words/s\n","2021-01-13 08:23:05,603 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-01-13 08:23:05,603 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-01-13 08:23:05,604 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-01-13 08:23:05,606 : INFO : EPOCH - 5 : training on 438 raw words (353 effective words) took 0.0s, 90896 effective words/s\n","2021-01-13 08:23:05,607 : INFO : training on a 2190 raw words (1779 effective words) took 0.0s, 39253 effective words/s\n","2021-01-13 08:23:05,609 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n","2021-01-13 08:23:05,610 : INFO : precomputing L2-norms of word weight vectors\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"PniPD48OiXd4"},"source":["Test it on one of the review sentences. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zKX5jPjjAK-p","executionInfo":{"status":"ok","timestamp":1610526222479,"user_tz":-480,"elapsed":808,"user":{"displayName":"Aobo Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1R4n3TLad19eGDFJcbKQpak37bv0BcjEpPoimgw=s64","userId":"18082252328038546601"}},"outputId":"81d6d805-5882-4613-daf6-ec5d39c20c8a"},"source":["start = time()\n","sent = 'love the gyro plate. Rice is so good and I also dig their candy selection.'\n","query = preprocess(sent)\n","sims = instance[query]  # A query is simply a \"look-up\" in the similarity class.\n","print ('Cell took %.2f seconds to run.' %(time() - start))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-01-13 08:23:42,026 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n","2021-01-13 08:23:42,028 : INFO : built Dictionary(76 unique tokens: ['absolute', 'absolutely', 'amazing', 'anyway', 'arrived']...) from 2 documents (total 85 corpus positions)\n","2021-01-13 08:23:42,047 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n","2021-01-13 08:23:42,048 : INFO : built Dictionary(90 unique tokens: ['arrived', 'awesome', 'back', 'bad', 'baked']...) from 2 documents (total 120 corpus positions)\n","2021-01-13 08:23:42,067 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n","2021-01-13 08:23:42,067 : INFO : built Dictionary(9 unique tokens: ['also', 'candy', 'dig', 'good', 'gyro']...) from 2 documents (total 18 corpus positions)\n","2021-01-13 08:23:42,071 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n","2021-01-13 08:23:42,073 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n","2021-01-13 08:23:42,074 : INFO : built Dictionary(43 unique tokens: ['albeit', 'always', 'assure', 'awesome', 'case']...) from 2 documents (total 46 corpus positions)\n","2021-01-13 08:23:42,084 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n","2021-01-13 08:23:42,085 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n","2021-01-13 08:23:42,086 : INFO : built Dictionary(163 unique tokens: ['aioli', 'almost', 'also', 'anew', 'another']...) from 2 documents (total 194 corpus positions)\n","2021-01-13 08:23:42,127 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n","2021-01-13 08:23:42,128 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n","2021-01-13 08:23:42,128 : INFO : built Dictionary(26 unique tokens: ['belly', 'buns', 'carefully', 'crafted', 'drink']...) from 2 documents (total 26 corpus positions)\n"],"name":"stderr"},{"output_type":"stream","text":["Cell took 0.11 seconds to run.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6klmHw9Diale"},"source":["Output the num_best sentences that are most similar to the query."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SWAt382zdPQy","executionInfo":{"status":"ok","timestamp":1610526243189,"user_tz":-480,"elapsed":829,"user":{"displayName":"Aobo Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1R4n3TLad19eGDFJcbKQpak37bv0BcjEpPoimgw=s64","userId":"18082252328038546601"}},"outputId":"82952a2d-34f3-4f28-a14e-f36a339e33d0"},"source":["# Print the query and the retrieved documents, together with their similarities.\n","print ('Query:', sent)\n","for i in range(num_best):\n","    print (i, '----')\n","    print (documents[sims[i][0]])\n","    print ('sim = %.4f' % sims[i][1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Query: love the gyro plate. Rice is so good and I also dig their candy selection.\n","0 ----\n","love the gyro plate. Rice is so good and I also dig their candy selection :)\n","sim = 1.0000\n","1 ----\n","I have no idea why some people give bad reviews about this place. It goes to show you, you can please everyone. They are probably griping about something that their own fault...there are many people like that.\n","\n","In any case, my friend and I arrived at about 5:50 PM this past Sunday. It was pretty crowded, more than I thought for a Sunday evening and thought we would have to wait forever to get a seat but they said we'll be seated when the girl comes back from seating someone else. We were seated at 5:52 and the waiter came and got our drink orders. Everyone was very pleasant from the host that seated us to the waiter to the server. The prices were very good as well. We placed our orders once we decided what we wanted at 6:02. We shared the baked spaghetti calzone and the small \"Here's The Beef\" pizza so we can both try them. The calzone was huge and we got the smallest one (personal) and got the small 11\" pizza. Both were awesome! My friend liked the pizza better and I liked the calzone better. The calzone does have a sweetish sauce but that's how I like my sauce!\n","\n","We had to box part of the pizza to take it home and we were out the door by 6:42. So, everything was great and not like these bad reviewers. That goes to show you that  you have to try these things yourself because all these bad reviewers have some serious issues.\n","sim = 0.4574\n","2 ----\n","Quiessence is, simply put, beautiful.  Full windows and earthy wooden walls give a feeling of warmth inside this restaurant perched in the middle of a farm.  The restaurant seemed fairly full even on a Tuesday evening; we had secured reservations just a couple days before.\n","\n","My friend and I had sampled sandwiches at the Farm Kitchen earlier that week, and were impressed enough to want to eat at the restaurant.  The crisp, fresh veggies didn't disappoint: we ordered the salad with orange and grapefruit slices and the crudites to start.  Both were very good; I didn't even know how much I liked raw radishes and turnips until I tried them with their pesto and aioli sauces.\n","\n","For entrees, I ordered the lamb and my friend ordered the pork shoulder.  Service started out very good, but trailed off quickly.  Waiting for our food took a very long time (a couple seated after us received and finished their entrees before we received our's), and no one bothered to explain the situation until the maitre'd apologized almost 45 minutes later.  Apparently the chef was unhappy with the sauce on my entree, so he started anew.  This isn't really a problem, but they should have communicated this to us earlier.  For our troubles, they comped me the glass of wine I ordered, but they forgot to bring out with my entree  as I had requested.  Also, they didn't offer us bread, but I will echo the lady who whispered this to us on her way out: ask for the bread.  We received warm foccacia, apple walnut, and pomegranate slices of wonder with honey and butter.  YUM.\n","\n","The entrees were both solid, but didn't quite live up to the innovation and freshness of the vegetables.  My lamb's sauce was delicious, but the meat was tough.  Maybe the vegetarian entrees are the way to go?  But our dessert, the gingerbread pear cake, was yet another winner.\n","\n","If the entrees were tad more inspired, or the service weren't so spotty, this place definitely would have warranted five stars.  If I return, I'd like to try the 75$ tasting menu.  Our bill came out to about 100$ for two people, including tip, no drinks.\n","sim = 0.4569\n","3 ----\n","My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n","\n","Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n","\n","While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n","\n","Anyway, I can't wait to go back!\n","sim = 0.4544\n","4 ----\n","Nobuo shows his unique talents with everything on the menu. Carefully crafted features with much to drink. Start with the pork belly buns and a stout. Then go on until you can no longer.\n","sim = 0.4505\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hwatTRYxwow3"},"source":["**Workshop submission:**\n","\n","Use the news.xls corpus to replace the yelp academic dataset to re-construct a word vector model. Let’s call this word vector model WV_News and the word vector model from Yelp reviews WV_Yelp. \n","\n","\n","*   Get the most similar document for the following two sentences from two models (Yelp & WV_NEWS):\n","\n","\n","sentence_1: The pandemic is showing little sign of slowing down, with more than 10,000 new deaths recorded worldwide every day. according to an AFP tally.\n","\n","Yelp doc:\n","\n","WV_NEWS doc:\n","\n","\n","---\n","\n","\n","Sentence_2:The dollar's weakening is likely to last at least another six months as investors continue to shift to risky assets and higher returns from a Reuters poll.\n","\n","Yelp doc:\n","\n","WV_NEWs doc:\n","\n","\n","\n","---\n","\n","\n","\n","*   Comment on your findings. For eg. On the choice of words, the usage/ size of the corpus – all these can affect the results). Does a different word vector corpus affect also the similarity scores? Also, on the ‘extracted’ news sentence outcome. Does it look suitable?\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YfIiffjtw8-v"},"source":[""]}]}