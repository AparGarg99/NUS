{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"wk_action_pytorch_v202107.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"S3tDSz7s3ZCe"},"source":["# Workshop on Real time video analytics (action recognition)\n","\n","Course: Real time audio visual sensing and sense making\n","\n","Website: https://www.iss.nus.edu.sg/executive-education/course/detail/real-time-audio-visual-sensing-and-sense--making/artificial-intelligence\n","\n","Contact: Tian Jing\n","\n","Email: tianjing@nus.edu.sg\n","\n","# Objective\n","In this workshop, we will perform the following three tasks\n","\n","- Exercise 1: Perform action recognition using histogram of optical flow\n","- Exercise 2: Perform action recognition using C3D deep learning approach\n","- Exercise 3: Perform action recognition using C3D + classifier\n","\n","# Submission guideline\n","\n","Once you finish the workshop, rename your .ipynb file to be your name, and submit your .ipynb file into LumiNUS."]},{"cell_type":"code","metadata":{"id":"3eZH1wWw3ZCf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619079899447,"user_tz":-480,"elapsed":1529,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"outputId":"a06dd922-1322-49ed-857d-ac62d6455cc0"},"source":["# Load library\n","import os\n","import cv2\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn import svm\n","from sklearn.metrics import confusion_matrix\n","\n","print(\"PyTorch version is\", torch.__version__)\n","# Use GPU if available else revert to CPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device being used:\", device)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["PyTorch version is 1.8.1+cu101\n","Device being used: cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IkzWnZdu3zP_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619079899448,"user_tz":-480,"elapsed":1525,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"outputId":"1a8abb87-6973-4448-bcfa-a7af84245b69"},"source":["# Grant access to google drive.\n","# Run this cell, then you’ll see a link, click on that link, allow access\n","# Copy the code that pops up, paste it in the box, hit Enter\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","# Change working directory to be current folder\n","import os\n","os.chdir('/content/gdrive/My Drive/RTAVS/action')\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","2_wk_action.zip\t\t    model_c3d_v0815.h5\n","archive\t\t\t    model_c3d_v0815.pt\n","data\t\t\t    wk_action_keras_v202107_reference.ipynb\n","data_test_hof_feature.npz   wk_action_pytorch_v202107_reference.ipynb\n","data_train_hof_feature.npz\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DQI-9bZt3ZCj"},"source":["# Explore the dataset\n","\n","- UCF11 Dataset: https://www.crcv.ucf.edu/data/UCF_YouTube_Action.php.\n","\n","It contains 11 action categories: basketball shooting, biking/cycling, diving, golf swinging, horse back riding, soccer juggling, swinging, tennis swinging, trampoline jumping, volleyball spiking, and walking with a dog.\n"]},{"cell_type":"code","metadata":{"id":"Xgx-bDIe3ZCj"},"source":["def load_groups(input_folder):\n","    '''\n","    Load the list of sub-folders into a python list with their\n","    corresponding label.\n","    '''\n","    groups         = []\n","    label_folders  = os.listdir(input_folder)\n","    index          = 0\n","    for label_folder in sorted(label_folders):\n","        label_folder_path = os.path.join(input_folder, label_folder)\n","        if os.path.isdir(label_folder_path):\n","            group_folders = os.listdir(label_folder_path)\n","            for group_folder in group_folders:\n","                if group_folder != 'Annotation':\n","                    groups.append([os.path.join(label_folder_path, group_folder), index])\n","            index += 1\n","\n","    return groups\n","\n","#Reference: https://github.com/microsoft/CNTK/blob/master/Examples/Video/DataSets/UCF11/split_ucf11.py\n","def ucf_split_data(groups, file_ext):\n","    '''\n","    Split the data at random for train, eval and test set.\n","    '''\n","    group_count = len(groups)\n","    indices = np.arange(group_count)\n","\n","    np.random.seed(0) # Make it deterministic.\n","    np.random.shuffle(indices)\n","\n","    # 80% training and 20% test.\n","    train_count = int(0.8 * group_count)\n","    test_count  = group_count - train_count\n","\n","    train = []\n","    test  = []\n","\n","    for i in range(train_count):\n","        group = groups[indices[i]]\n","        video_files = os.listdir(group[0])\n","        for video_file in video_files:\n","            video_file_path = os.path.join(group[0], video_file)\n","            if os.path.isfile(video_file_path):\n","                video_file_path = os.path.abspath(video_file_path)\n","                ext = os.path.splitext(video_file_path)[1]\n","                if (ext == file_ext):\n","                    train.append([video_file_path, group[1]])\n","\n","    for i in range(train_count, train_count + test_count):\n","        group = groups[indices[i]]\n","        video_files = os.listdir(group[0])\n","        for video_file in video_files:\n","            video_file_path = os.path.join(group[0], video_file)\n","            if os.path.isfile(video_file_path):\n","                video_file_path = os.path.abspath(video_file_path)\n","                ext = os.path.splitext(video_file_path)[1]\n","                if (ext == file_ext):\n","                    test.append([video_file_path, group[1]])\n","\n","    return train, test\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XM6_ZF3B3ZCm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619079899983,"user_tz":-480,"elapsed":2049,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"outputId":"769d0fe1-e49b-4116-a793-9c25bb186c32"},"source":["# Prepare the dataset\n","ucf_groups = load_groups(\"data\")\n","\n","ucf_action_labels  = os.listdir(\"data\")\n","print(\"action labels: \", ucf_action_labels)\n","\n","ucf_train, ucf_test = ucf_split_data(ucf_groups, '.avi')\n","print(\"Total %d categories, Training data %d sequences, test data %d sequences\" % (len(ucf_action_labels), len(ucf_train), len(ucf_test)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["action labels:  ['basketball', 'biking', 'diving', 'tennis_swing', 'volleyball_spiking', 'golf_swing', 'horse_riding', 'soccer_juggling', 'trampoline_jumping', 'swing', 'walking']\n","Total 11 categories, Training data 1295 sequences, test data 305 sequences\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q-E54zWM3ZCp"},"source":["# Exercise 1: Action recognition using histogram of optical flow\n","\n","- Reference: Histogram of optical flow,  https://github.com/colincsl/pyKinectTools/blob/master/pyKinectTools/algs/HistogramOfOpticalFlow.py"]},{"cell_type":"code","metadata":{"id":"dkEcVRtI3ZCp"},"source":["# Reference: https://github.com/colincsl/pyKinectTools/blob/master/pyKinectTools/algs/HistogramOfOpticalFlow.py\n","# Fix a few bugs\n","def hof(flow, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), normalise=False, motion_threshold=1.):\n","\n","    \"\"\"Extract Histogram of Optical Flow (HOF) for a given image.\n","    Key difference between this and HOG is that flow is MxNx2 instead of MxN\n","    Compute a Histogram of Optical Flow (HOF) by\n","        1. (optional) global image normalisation\n","        2. computing the dense optical flow\n","        3. computing flow histograms\n","        4. normalising across blocks\n","        5. flattening into a feature vector\n","    Parameters\n","    ----------\n","    Flow : (M, N) ndarray\n","        Input image (x and y flow images).\n","    orientations : int\n","        Number of orientation bins.\n","    pixels_per_cell : 2 tuple (int, int)\n","        Size (in pixels) of a cell.\n","    cells_per_block  : 2 tuple (int,int)\n","        Number of cells in each block.\n","    normalise : bool, optional\n","        Apply power law compression to normalise the image before\n","        processing.\n","    static_threshold : threshold for no motion\n","    Returns\n","    -------\n","    newarr : ndarray\n","        hof for the image as a 1D (flattened) array.\n","    hof_image : ndarray (if visualise=True)\n","        A visualisation of the hof image.\n","    References\n","    ----------\n","    * http://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n","    * Dalal, N and Triggs, B, Histograms of Oriented Gradients for\n","      Human Detection, IEEE Computer Society Conference on Computer\n","      Vision and Pattern Recognition 2005 San Diego, CA, USA\n","    \"\"\"\n","    flow = np.atleast_2d(flow)\n","\n","    \"\"\" \n","    -1-\n","    The first stage applies an optional global image normalisation\n","    equalisation that is designed to reduce the influence of illumination\n","    effects. In practice we use gamma (power law) compression, either\n","    computing the square root or the log of each colour channel.\n","    Image texture strength is typically proportional to the local surface\n","    illumination so this compression helps to reduce the effects of local\n","    shadowing and illumination variations.\n","    \"\"\"\n","\n","    if flow.ndim < 3:\n","        raise ValueError(\"Requires dense flow in both directions\")\n","\n","    if normalise:\n","        flow = sqrt(flow)\n","\n","    \"\"\" \n","    -2-\n","    The second stage computes first order image gradients. These capture\n","    contour, silhouette and some texture information, while providing\n","    further resistance to illumination variations. The locally dominant\n","    colour channel is used, which provides colour invariance to a large\n","    extent. Variant methods may also include second order image derivatives,\n","    which act as primitive bar detectors - a useful feature for capturing,\n","    e.g. bar like structures in bicycles and limbs in humans.\n","    \"\"\"\n","\n","    if flow.dtype.kind == 'u':\n","        # convert uint image to float\n","        # to avoid problems with subtracting unsigned numbers in np.diff()\n","        flow = flow.astype('float')\n","\n","    gx = np.zeros(flow.shape[:2])\n","    gy = np.zeros(flow.shape[:2])\n","    # gx[:, :-1] = np.diff(flow[:,:,1], n=1, axis=1)\n","    # gy[:-1, :] = np.diff(flow[:,:,0], n=1, axis=0)\n","\n","    gx = flow[:,:,1]\n","    gy = flow[:,:,0]\n","\n","\n","    \"\"\" \n","    -3-\n","    The third stage aims to produce an encoding that is sensitive to\n","    local image content while remaining resistant to small changes in\n","    pose or appearance. The adopted method pools gradient orientation\n","    information locally in the same way as the SIFT [Lowe 2004]\n","    feature. The image window is divided into small spatial regions,\n","    called \"cells\". For each cell we accumulate a local 1-D histogram\n","    of gradient or edge orientations over all the pixels in the\n","    cell. This combined cell-level 1-D histogram forms the basic\n","    \"orientation histogram\" representation. Each orientation histogram\n","    divides the gradient angle range into a fixed number of\n","    predetermined bins. The gradient magnitudes of the pixels in the\n","    cell are used to vote into the orientation histogram.\n","    \"\"\"\n","\n","    magnitude = sqrt(gx**2 + gy**2)\n","    orientation = arctan2(gy, gx) * (180 / pi) % 180\n","\n","    sy, sx = flow.shape[:2]\n","    cx, cy = pixels_per_cell\n","    bx, by = cells_per_block\n","\n","    n_cellsx = int(np.floor(sx // cx))  # number of cells in x\n","    n_cellsy = int(np.floor(sy // cy))  # number of cells in y\n","\n","    # compute orientations integral images\n","    orientation_histogram = np.zeros((n_cellsy, n_cellsx, orientations))\n","    subsample = np.index_exp[int(cy / 2):cy * n_cellsy:cy, int(cx / 2):cx * n_cellsx:cx]\n","    # There are (orientations-1) bins for optical flow and 1 bin for no-motion\n","    for i in range(orientations-1):\n","        #create new integral image for this orientation\n","        # isolate orientations in this range\n","\n","        # temp_ori = np.where(orientation < 180 / orientations * (i + 1), orientation, -1)\n","        # temp_ori = np.where(orientation >= 180 / orientations * i, temp_ori, -1)\n","        # fixed the bug in the original Github code\n","        temp_ori = np.where(orientation < 180 / (orientations-1) * (i + 1), orientation, -1)\n","        temp_ori = np.where(orientation >= 180 / (orientations-1) * i, temp_ori, -1)\n","        # select magnitudes for those orientations\n","        cond2 = (temp_ori > -1) * (magnitude > motion_threshold)\n","        temp_mag = np.where(cond2, magnitude, 0)\n","\n","        temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n","        orientation_histogram[:, :, i] = temp_filt[subsample]\n","\n","    ''' Calculate the no-motion bin '''\n","    temp_mag = np.where(magnitude <= motion_threshold, magnitude, 0)\n","\n","    temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n","    orientation_histogram[:, :, -1] = temp_filt[subsample]\n","\n","    \"\"\"\n","    The fourth stage computes normalisation, which takes local groups of\n","    cells and contrast normalises their overall responses before passing\n","    to next stage. Normalisation introduces better invariance to illumination,\n","    shadowing, and edge contrast. It is performed by accumulating a measure\n","    of local histogram \"energy\" over local groups of cells that we call\n","    \"blocks\". The result is used to normalise each cell in the block.\n","    Typically each individual cell is shared between several blocks, but\n","    its normalisations are block dependent and thus different. The cell\n","    thus appears several times in the final output vector with different\n","    normalisations. This may seem redundant but it improves the performance.\n","    We refer to the normalised block descriptors as Histogram of Oriented\n","    Gradient (hog) descriptors.\n","    \"\"\"\n","\n","    n_blocksx = (n_cellsx - bx) + 1\n","    n_blocksy = (n_cellsy - by) + 1\n","    normalised_blocks = np.zeros((n_blocksy, n_blocksx,\n","                                  by, bx, orientations))\n","\n","    for x in range(n_blocksx):\n","        for y in range(n_blocksy):\n","            block = orientation_histogram[y:y+by, x:x+bx, :]\n","            eps = 1e-5\n","            normalised_blocks[y, x, :] = block / sqrt(block.sum()**2 + eps)\n","\n","    return normalised_blocks.ravel()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4TF7vqH3ZCs"},"source":["# Define the HoF feature extraction function\n","def extract_hof_feature(video_list):\n","    feature_hof = []\n","    label_list = []\n","    img_width = 128\n","    img_height = 64\n","    for idx, value in enumerate(video_list):\n","        # Display the progress\n","        if (idx % 100) == 0:\n","            print(\"process sequence %d/%d\" % (idx, len(video_list)))\n","        filename = value[0]\n","        label = value[1]\n","        hof_feature_all = []\n","\n","        cap = cv2.VideoCapture(filename)\n","        ret, frame = cap.read()\n","        if ret:\n","            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","            gray = cv2.resize(gray, (img_width, img_height)) # Resize frames to reduce feature dimensions\n","        \n","            while True:\n","                previousGray = gray\n","                ret, frame = cap.read()\n","\n","                if ret:\n","                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","                    gray = cv2.resize(gray, (img_width, img_height))\n","                    flow = cv2.calcOpticalFlowFarneback(previousGray, gray, flow=None, pyr_scale=0.5, levels=5, winsize=11, iterations=10, poly_n=5, poly_sigma=1.1, flags=0)\n","                    hof_feature_one = hof(flow, orientations=9, pixels_per_cell=(8, 8),cells_per_block=(2, 2))\n","                    if (len(hof_feature_all) == 0):\n","                        hof_feature_all = hof_feature_one\n","                    else:\n","                        hof_feature_all = np.vstack((hof_feature_all, hof_feature_one))\n","                else:\n","                    break\n","    \n","        cap.release()\n","        if (len(hof_feature_all) != 0):\n","            hof_feature_mean = np.mean(hof_feature_all, axis=0)\n","            feature_hof.append(hof_feature_mean)\n","            label_list.append(label)      \n","        \n","    return np.array(feature_hof), np.array(label_list)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CC0_4bBx3ZCu"},"source":["# It takes around half hour to prepare such feature dataset\n","# You can uncomment if you want to re-build feature dataset, otherwise, load them from the data files\n","# The sequence file basketball\\v_shooting_24\\v_shooting_24_01.avi is very short.\n","\n","# print(\"Prepare training feature dataset\")\n","# train_feature_hof, train_label_hof = extract_hof_feature(ucf_train)\n","# np.savez(\"data_train_hof_feature.npz\", X=train_feature_hof, Y=train_label_hof)\n","# print(train_feature_hof.shape, train_label_hof.shape)\n","\n","# print(\"Prepare test feature dataset\")\n","# test_feature_hof, test_label_hof = extract_hof_feature(ucf_test)\n","# np.savez(\"data_test_hof_feature.npz\", X=test_feature_hof, Y=test_label_hof)\n","# print(test_feature_hof.shape, test_label_hof.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R5POsmAh3ZCx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619079900390,"user_tz":-480,"elapsed":2434,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"outputId":"c5387811-a60b-4566-de04-95edcab172af"},"source":["# Load HoF features from pre-prepared data file and perform SVM classification\n","with np.load(\"data_train_hof_feature.npz\") as npzfile:\n","    x_train_hof = npzfile[\"X\"]\n","    x_train_hof_label = npzfile[\"Y\"]\n","    \n","with np.load(\"data_test_hof_feature.npz\") as npzfile:\n","    x_test_hof = npzfile[\"X\"]\n","    x_test_hof_label = npzfile[\"Y\"]\n","    \n","print(\"Training data\", x_train_hof.shape, x_train_hof_label.shape)\n","print(\"Test data\", x_test_hof.shape, x_test_hof_label.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training data (1294, 3780) (1294,)\n","Test data (305, 3780) (305,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-B_wJ3xB3ZC0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619079907653,"user_tz":-480,"elapsed":9690,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"outputId":"14dbc685-3904-4932-d730-3e1f66e53b04"},"source":["hof_svm_model = svm.SVC(kernel = 'linear', C = 10).fit(x_train_hof, x_train_hof_label)\n","\n","x_test_hof_pred = hof_svm_model.predict(x_test_hof)\n","\n","print(confusion_matrix(x_test_hof_label, x_test_hof_pred))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 0  1  2  0  0  0  0  0  0  0  2]\n"," [ 7 12  1  0  7  0  9  0  0  0  7]\n"," [ 2  0 10  0  0  0  0  0  0  0  2]\n"," [ 0  0  6 21  1  2  1  0  1  2  0]\n"," [ 5  2  3  0  9  0  0  0  0  0  1]\n"," [ 5  0  9  2  0  6  1  5  5  0  0]\n"," [ 0  2  5  0  0  3 10  0  3  1  5]\n"," [ 4  2  1 12  0  9  0 20  1  3  0]\n"," [ 5  0  0  0  0  0  1  0 14  2  1]\n"," [ 3  0  4  1  0  0  0  1  0  5  4]\n"," [ 2  5  2  2  7  0  1  0  0  0 15]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yEdUiVlb3ZC2"},"source":["# Exercise 2: Action recognition using C3D model\n","\n","A modified C3D model is used in the workshop to reduce model training time for demonstration purpose.\n","\n","- Reference: D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning Spatiotemporal Features with 3D Convolutional Networks“, ICCV 2015, https://arxiv.org/abs/1412.0767"]},{"cell_type":"code","metadata":{"id":"wijRprjc3ZC3"},"source":["class Videoto3D:\n","\n","    def __init__(self, width, height, depth):\n","        self.width = width\n","        self.height = height\n","        self.depth = depth\n","\n","    def get_data(self, filename, skip=True):\n","        cap = cv2.VideoCapture(filename)\n","        nframe = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n","        bAppend = False\n","        if (nframe>=self.depth):\n","            if skip:\n","                frames = [x * nframe / self.depth for x in range(self.depth)]\n","            else:\n","                frames = [x for x in range(self.depth)]\n","        else:\n","            print(\"Insufficient %d frames in video %s, set bAppend as True\" % (nframe, filename))\n","            bAppend = True\n","            frames = [x for x in range(int(nframe))] # nframe is a float\n","\n","        framearray = []\n","\n","        for i in range(len(frames)):#self.depth):\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, frames[i])\n","            ret, frame = cap.read()\n","            frame = cv2.resize(frame, (self.height, self.width))\n","            framearray.append(frame)\n","\n","        cap.release()\n","        \n","        if bAppend:\n","            while len(framearray) < self.depth:\n","                framearray.append(frame)\n","            print(\"Append more frames in the framearray to have %d frames\" % len(framearray))\n","                \n","        return np.array(framearray)\n","\n","def loaddata(video_list, vid3d, skip=True):\n","    X = []\n","    Y = []\n","    for idx, value in enumerate(video_list):\n","        # Display the progress\n","        if (idx % 100) == 0:\n","            print(\"process data %d/%d\" % (idx, len(video_list)))\n","        filename = value[0]\n","        label = value[1]\n","        Y.append(label)\n","        X.append(vid3d.get_data(filename, skip=skip))\n","\n","    return np.array(X), np.array(Y)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQQ8eVRf3ZC5"},"source":["# Define parameter setting\n","class Args:\n","    nclass = len(ucf_action_labels) # 11 action categories\n","    depth = 10\n","    rows = 32\n","    cols = 32\n","    skip = True # Skip: randomly extract frames; otherwise, extract first few frames\n","    color_channel = 3\n","\n","param_setting = Args()\n","img_rows = param_setting.rows\n","img_cols = param_setting.cols\n","frames = param_setting.depth\n","vid3d = Videoto3D(img_rows, img_cols, frames)\n","nb_classes = param_setting.nclass\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FD3kE9Rr3ZC8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619079949068,"user_tz":-480,"elapsed":51093,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"outputId":"7c390293-8e2f-4b95-f60f-2e98ebdc72a3"},"source":["# Prepare training data\n","x_train, y_train = loaddata(ucf_train, vid3d, param_setting.skip)\n","\n","# Prepare test data\n","x_test, y_test = loaddata(ucf_test, vid3d, param_setting.skip)\n","\n","print(\"Training data\", x_train.shape, y_train.shape)\n","print(\"Test data\", x_test.shape, y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["process data 0/1295\n","process data 100/1295\n","process data 200/1295\n","process data 300/1295\n","process data 400/1295\n","process data 500/1295\n","process data 600/1295\n","process data 700/1295\n","process data 800/1295\n","process data 900/1295\n","process data 1000/1295\n","Insufficient 1 frames in video /content/gdrive/My Drive/RTAVS/action/data/basketball/v_shooting_24/v_shooting_24_01.avi, set bAppend as True\n","Append more frames in the framearray to have 10 frames\n","process data 1100/1295\n","process data 1200/1295\n","process data 0/305\n","process data 100/305\n","process data 200/305\n","process data 300/305\n","Training data (1295, 10, 32, 32, 3) (1295,)\n","Test data (305, 10, 32, 32, 3) (305,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ohjXjMFRRWik"},"source":["\n","class myDataSet(torch.utils.data.Dataset):\n","    def __init__(self, data_X, data_Y, nb_classes):\n","        self.X = data_X.astype('float32')/255.0\n","        self.X = self.X.transpose(0, 4, 1, 2, 3) # take note the dimension used in model training\n","        self.Y =torch.from_numpy(data_Y)\n","        self.num_samples = self.Y.shape[0]\n","\n","    def __getitem__(self, index):\n","        return self.X[index], self.Y[index]   \n","\n","    def __len__(self):\n","        return self.num_samples\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gVvGb1nJUHqF"},"source":["class C3D(nn.Module):\n","    # A simplified C3D model\n","\n","    def __init__(self, num_classes):\n","        super(C3D, self).__init__()\n","\n","        self.conv1 = nn.Conv3d(3, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n","        self.conv2 = nn.Conv3d(32, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n","        self.pool2 = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=(2, 2, 2))\n","\n","        self.conv3a = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n","        self.conv3b = nn.Conv3d(64, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n","        self.pool3 = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=(2, 2, 2))\n","\n","        self.fc4 = nn.Linear(3136, 512)\n","        self.fc5 = nn.Linear(512, num_classes)\n","\n","        self.dropout = nn.Dropout(p=0.2)\n","        self.relu = nn.ReLU()\n","        self.__init_weight()\n","\n","    def forward(self, x):\n","\n","        x = self.relu(self.conv1(x))\n","        x = self.relu(self.conv2(x))\n","        x = self.pool2(x)\n","        x = self.dropout(x)\n","\n","        x = self.relu(self.conv3a(x))\n","        x = self.relu(self.conv3b(x))\n","        x = self.pool3(x)\n","        x = self.dropout(x)\n","\n","        x = x.view(x.size()[0], -1)\n","        x = self.relu(self.fc4(x))\n","        x = self.dropout(x)\n","        logits = self.fc5(x)\n","        \n","        return x, logits # the 'x' will be used as features later, the logits will be used as model output\n","\n","    def __init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv3d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm3d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OyohGJWcZzhk"},"source":["\n","def train_model(model, batch_size, lr, num_epochs):\n","      \n","    criterion = nn.CrossEntropyLoss()  # standard crossentropy loss for classification\n","    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n","    train_dataloader = torch.utils.data.DataLoader(myDataSet(x_train, y_train, nb_classes), shuffle=True, batch_size=batch_size)\n","\n","    model.to(device)\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        model.train()\n","\n","        for idx, (inputs, labels) in enumerate(train_dataloader):\n","            if (epoch ==0) & (idx == 0):\n","                print(inputs.shape, labels.shape)        #  Check dimension of a batch of training data\n","                                        \n","            inputs = inputs.to(device)\n","            labels = labels.to(device, dtype=torch.int64)\n","            optimizer.zero_grad()\n","            _, outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","        epoch_loss = running_loss / len(train_dataloader)\n","\n","        if ((epoch % 20) == 0):\n","            print(\"Epoch: %s/%s, Loss: %.4f\" % (epoch+1, num_epochs, epoch_loss))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gQoarBrXaP5T","executionInfo":{"status":"ok","timestamp":1619079952061,"user_tz":-480,"elapsed":54070,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"outputId":"6a61fe59-ffcd-409f-cc9a-996e006a6f68"},"source":["num_epochs = 400  # Number of epochs for training\n","lr = 1e-4 # Learning rate\n","batch_size = 32\n","\n","model = C3D(num_classes=nb_classes)\n","# train_model(model=model, batch_size=batch_size, lr=lr, num_epochs=num_epochs)\n","# torch.save(model.state_dict(), \"model_c3d_v0815.pt\")\n","\n","# Load models\n","loaded_model = C3D(num_classes=nb_classes)\n","loaded_model.load_state_dict(torch.load(\"model_c3d_v0815.pt\"))\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HUUn77Yjrarx","executionInfo":{"status":"ok","timestamp":1619079952783,"user_tz":-480,"elapsed":54788,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"outputId":"31d3a010-7b97-43f0-ba96-11f4bc098c8a"},"source":["batch_size = 10\n","loaded_model.to(device)\n","\n","test_dataloader  = torch.utils.data.DataLoader(myDataSet(x_test, y_test, nb_classes), batch_size=batch_size)\n","test_size = len(test_dataloader.dataset)\n","conf_mat = np.zeros([nb_classes, nb_classes])\n","\n","loaded_model.eval()\n","with torch.no_grad():\n","    for idx, (inputs, labels) in enumerate(test_dataloader):\n","\n","        inputs = inputs.to(device)\n","        labels = labels.to(device, dtype=torch.int64)\n","        _, outputs = loaded_model(inputs)\n","\n","        probs = nn.Softmax(dim=1)(outputs)\n","        preds = torch.max(probs, 1)[1]\n","\n","        for idx1 in range(preds.shape[0]):\n","            ii = labels[idx1].item()\n","            jj = preds[idx1].item()\n","            conf_mat[ii, jj] += 1.0\n","\n","print(conf_mat)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 2.  0.  0.  0.  1.  0.  0.  0.  0.  1.  1.]\n"," [ 2. 16.  0.  2.  6.  0.  5.  1.  2.  1.  8.]\n"," [ 0.  0. 14.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0. 33.  0.  0.  0.  0.  0.  0.  1.]\n"," [ 0.  1.  0.  0. 16.  0.  2.  1.  0.  0.  0.]\n"," [ 0.  2.  0. 20.  1.  5.  0.  1.  0.  2.  2.]\n"," [ 0.  3.  1.  1.  2.  0.  9.  0.  5.  5.  3.]\n"," [ 0.  0.  3.  9.  0.  7.  0. 24.  0.  5.  4.]\n"," [ 0.  0.  1.  0.  0.  0.  3.  6. 12.  0.  1.]\n"," [ 2.  0.  0.  0.  1.  0.  0.  0.  1. 14.  0.]\n"," [ 0. 11.  0.  2.  5.  0.  3.  1.  0.  0. 12.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DWBBhV0w3ZDG"},"source":["$\\color{red}{\\text{Q1: Complete code to perform action recognition using C3D + classifier}}$\n","\n","Tasks\n","- Extract the fully connected layer response `flatten_feature` from the C3D model `c3d_model` as features from the training data `x_train`\n","- Labels are provided in `ucf_train` and `ucf_test`\n","- Build a classification model, such as SVM\n","- Perform classification on `x_test`, display the confusion matrix\n"]},{"cell_type":"code","metadata":{"id":"4W943NjK3ZDG"},"source":["# Provide your solution to Q1 here\n","#\n","# "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kJbZZcKfuoo6"},"source":["$\\color{red}{\\text{Q2: Propose how to apply the model developed in this workshop on the live video streaming.}}$\n","\n","The model developed in this workshop works on a short video clip. In practice, given a live video streaming, how to apply such model for action recognition?"]},{"cell_type":"code","metadata":{"id":"q9wqi3V2un3x"},"source":["# Provide your solution to Q2 here (no need programming)\n","#\n","#"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"43UZE2MD3ZDI"},"source":["**Once you finish the workshop, rename your .ipynb file to be your name, and submit your .ipynb file into LumiNUS.**\n","\n","Have a nice day!"]}]}