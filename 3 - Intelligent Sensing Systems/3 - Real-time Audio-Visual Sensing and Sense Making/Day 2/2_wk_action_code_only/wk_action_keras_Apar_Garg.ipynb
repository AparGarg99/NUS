{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3tDSz7s3ZCe"
   },
   "source": [
    "# Workshop on Real time video analytics (action recognition)\n",
    "\n",
    "Course: Real time audio visual sensing and sense making\n",
    "\n",
    "Website: https://www.iss.nus.edu.sg/executive-education/course/detail/real-time-audio-visual-sensing-and-sense--making/artificial-intelligence\n",
    "\n",
    "Contact: Tian Jing\n",
    "\n",
    "Email: tianjing@nus.edu.sg\n",
    "\n",
    "# Objective\n",
    "In this workshop, we will perform the following three tasks\n",
    "\n",
    "- Exercise 1: Perform action recognition using histogram of optical flow\n",
    "- Exercise 2: Perform action recognition using C3D deep learning approach\n",
    "- Exercise 3: Perform action recognition using C3D + classifier\n",
    "\n",
    "# Submission guideline\n",
    "\n",
    "Once you finish the workshop, rename your .ipynb file to be your name, and submit your .ipynb file into LumiNUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eZH1wWw3ZCf",
    "outputId": "23481d62-d4e4-4195-fb4c-842fbb31c7e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.6.0\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import sqrt, pi, arctan2, cos, sin # used for HoF\n",
    "from scipy.ndimage import uniform_filter # used for hoF\n",
    "from sklearn import svm\n",
    "\n",
    "from tensorflow.keras.layers import Activation, Conv3D, Dense, Dropout, Flatten, MaxPooling3D\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "# Check GPU coinfiguration in Colab\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IkzWnZdu3zP_",
    "outputId": "a15cabe0-3328-4217-f91b-e4e9e3f68cd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      " data\n",
      " data_test_hof_feature.npz\n",
      " data_train_hof_feature.npz\n",
      "'Day 2 workshop video dataset download link (google drive).txt'\n",
      " model_c3d_v0815.h5\n",
      " model_c3d_v0815.pt\n",
      "'wk_action_keras_Apar Garg.ipynb'\n"
     ]
    }
   ],
   "source": [
    "# Grant access to google drive.\n",
    "# Run this cell, then youâ€™ll see a link, click on that link, allow access\n",
    "# Copy the code that pops up, paste it in the box, hit Enter\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "# Change working directory to be current folder\n",
    "import os\n",
    "os.chdir('/content/gdrive/MyDrive/iss/RTAVS/action')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQI-9bZt3ZCj"
   },
   "source": [
    "# Explore the dataset\n",
    "\n",
    "- UCF11 Dataset: https://www.crcv.ucf.edu/data/UCF_YouTube_Action.php.\n",
    "\n",
    "It contains 11 action categories: basketball shooting, biking/cycling, diving, golf swinging, horse back riding, soccer juggling, swinging, tennis swinging, trampoline jumping, volleyball spiking, and walking with a dog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Xgx-bDIe3ZCj"
   },
   "outputs": [],
   "source": [
    "def load_groups(input_folder):\n",
    "    '''\n",
    "    Load the list of sub-folders into a python list with their\n",
    "    corresponding label.\n",
    "    '''\n",
    "    groups         = []\n",
    "    label_folders  = os.listdir(input_folder)\n",
    "    index          = 0\n",
    "    for label_folder in sorted(label_folders):\n",
    "        label_folder_path = os.path.join(input_folder, label_folder)\n",
    "        if os.path.isdir(label_folder_path):\n",
    "            group_folders = os.listdir(label_folder_path)\n",
    "            for group_folder in group_folders:\n",
    "                if group_folder != 'Annotation':\n",
    "                    groups.append([os.path.join(label_folder_path, group_folder), index])\n",
    "            index += 1\n",
    "\n",
    "    return groups\n",
    "\n",
    "#Reference: https://github.com/microsoft/CNTK/blob/master/Examples/Video/DataSets/UCF11/split_ucf11.py\n",
    "def ucf_split_data(groups, file_ext):\n",
    "    '''\n",
    "    Split the data at random for train, eval and test set.\n",
    "    '''\n",
    "    group_count = len(groups)\n",
    "    indices = np.arange(group_count)\n",
    "\n",
    "    np.random.seed(0) # Make it deterministic.\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # 80% training and 20% test.\n",
    "    train_count = int(0.8 * group_count)\n",
    "    test_count  = group_count - train_count\n",
    "\n",
    "    train = []\n",
    "    test  = []\n",
    "\n",
    "    for i in range(train_count):\n",
    "        group = groups[indices[i]]\n",
    "        video_files = os.listdir(group[0])\n",
    "        for video_file in video_files:\n",
    "            video_file_path = os.path.join(group[0], video_file)\n",
    "            if os.path.isfile(video_file_path):\n",
    "                video_file_path = os.path.abspath(video_file_path)\n",
    "                ext = os.path.splitext(video_file_path)[1]\n",
    "                if (ext == file_ext):\n",
    "                    train.append([video_file_path, group[1]])\n",
    "\n",
    "    for i in range(train_count, train_count + test_count):\n",
    "        group = groups[indices[i]]\n",
    "        video_files = os.listdir(group[0])\n",
    "        for video_file in video_files:\n",
    "            video_file_path = os.path.join(group[0], video_file)\n",
    "            if os.path.isfile(video_file_path):\n",
    "                video_file_path = os.path.abspath(video_file_path)\n",
    "                ext = os.path.splitext(video_file_path)[1]\n",
    "                if (ext == file_ext):\n",
    "                    test.append([video_file_path, group[1]])\n",
    "\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XM6_ZF3B3ZCm",
    "outputId": "b98c973e-b853-42fe-b5b3-ab79b418e683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action labels:  ['soccer_juggling', 'basketball', 'trampoline_jumping', 'biking', 'swing', 'volleyball_spiking', 'diving', 'tennis_swing', 'horse_riding', 'golf_swing', 'walking', 'model_c3d_v0815.h5', 'model.hdf5']\n",
      "Total 13 categories, Training data 1292 sequences, test data 308 sequences\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "ucf_groups = load_groups(\"data\")\n",
    "\n",
    "ucf_action_labels  = os.listdir(\"data\")\n",
    "print(\"action labels: \", ucf_action_labels)\n",
    "\n",
    "ucf_train, ucf_test = ucf_split_data(ucf_groups, '.avi')\n",
    "print(\"Total %d categories, Training data %d sequences, test data %d sequences\" % (len(ucf_action_labels), len(ucf_train), len(ucf_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-E54zWM3ZCp"
   },
   "source": [
    "# Exercise 1: Action recognition using histogram of optical flow\n",
    "\n",
    "- Reference: Histogram of optical flow,  https://github.com/colincsl/pyKinectTools/blob/master/pyKinectTools/algs/HistogramOfOpticalFlow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dkEcVRtI3ZCp"
   },
   "outputs": [],
   "source": [
    "# Reference: https://github.com/colincsl/pyKinectTools/blob/master/pyKinectTools/algs/HistogramOfOpticalFlow.py\n",
    "# Fix a few bugs\n",
    "def hof(flow, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), normalise=False, motion_threshold=1.):\n",
    "\n",
    "    \"\"\"Extract Histogram of Optical Flow (HOF) for a given image.\n",
    "    Key difference between this and HOG is that flow is MxNx2 instead of MxN\n",
    "    Compute a Histogram of Optical Flow (HOF) by\n",
    "        1. (optional) global image normalisation\n",
    "        2. computing the dense optical flow\n",
    "        3. computing flow histograms\n",
    "        4. normalising across blocks\n",
    "        5. flattening into a feature vector\n",
    "    Parameters\n",
    "    ----------\n",
    "    Flow : (M, N) ndarray\n",
    "        Input image (x and y flow images).\n",
    "    orientations : int\n",
    "        Number of orientation bins.\n",
    "    pixels_per_cell : 2 tuple (int, int)\n",
    "        Size (in pixels) of a cell.\n",
    "    cells_per_block  : 2 tuple (int,int)\n",
    "        Number of cells in each block.\n",
    "    normalise : bool, optional\n",
    "        Apply power law compression to normalise the image before\n",
    "        processing.\n",
    "    static_threshold : threshold for no motion\n",
    "    Returns\n",
    "    -------\n",
    "    newarr : ndarray\n",
    "        hof for the image as a 1D (flattened) array.\n",
    "    hof_image : ndarray (if visualise=True)\n",
    "        A visualisation of the hof image.\n",
    "    References\n",
    "    ----------\n",
    "    * http://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n",
    "    * Dalal, N and Triggs, B, Histograms of Oriented Gradients for\n",
    "      Human Detection, IEEE Computer Society Conference on Computer\n",
    "      Vision and Pattern Recognition 2005 San Diego, CA, USA\n",
    "    \"\"\"\n",
    "    flow = np.atleast_2d(flow)\n",
    "\n",
    "    \"\"\" \n",
    "    -1-\n",
    "    The first stage applies an optional global image normalisation\n",
    "    equalisation that is designed to reduce the influence of illumination\n",
    "    effects. In practice we use gamma (power law) compression, either\n",
    "    computing the square root or the log of each colour channel.\n",
    "    Image texture strength is typically proportional to the local surface\n",
    "    illumination so this compression helps to reduce the effects of local\n",
    "    shadowing and illumination variations.\n",
    "    \"\"\"\n",
    "\n",
    "    if flow.ndim < 3:\n",
    "        raise ValueError(\"Requires dense flow in both directions\")\n",
    "\n",
    "    if normalise:\n",
    "        flow = sqrt(flow)\n",
    "\n",
    "    \"\"\" \n",
    "    -2-\n",
    "    The second stage computes first order image gradients. These capture\n",
    "    contour, silhouette and some texture information, while providing\n",
    "    further resistance to illumination variations. The locally dominant\n",
    "    colour channel is used, which provides colour invariance to a large\n",
    "    extent. Variant methods may also include second order image derivatives,\n",
    "    which act as primitive bar detectors - a useful feature for capturing,\n",
    "    e.g. bar like structures in bicycles and limbs in humans.\n",
    "    \"\"\"\n",
    "\n",
    "    if flow.dtype.kind == 'u':\n",
    "        # convert uint image to float\n",
    "        # to avoid problems with subtracting unsigned numbers in np.diff()\n",
    "        flow = flow.astype('float')\n",
    "\n",
    "    gx = np.zeros(flow.shape[:2])\n",
    "    gy = np.zeros(flow.shape[:2])\n",
    "    # gx[:, :-1] = np.diff(flow[:,:,1], n=1, axis=1)\n",
    "    # gy[:-1, :] = np.diff(flow[:,:,0], n=1, axis=0)\n",
    "\n",
    "    gx = flow[:,:,1]\n",
    "    gy = flow[:,:,0]\n",
    "\n",
    "\n",
    "    \"\"\" \n",
    "    -3-\n",
    "    The third stage aims to produce an encoding that is sensitive to\n",
    "    local image content while remaining resistant to small changes in\n",
    "    pose or appearance. The adopted method pools gradient orientation\n",
    "    information locally in the same way as the SIFT [Lowe 2004]\n",
    "    feature. The image window is divided into small spatial regions,\n",
    "    called \"cells\". For each cell we accumulate a local 1-D histogram\n",
    "    of gradient or edge orientations over all the pixels in the\n",
    "    cell. This combined cell-level 1-D histogram forms the basic\n",
    "    \"orientation histogram\" representation. Each orientation histogram\n",
    "    divides the gradient angle range into a fixed number of\n",
    "    predetermined bins. The gradient magnitudes of the pixels in the\n",
    "    cell are used to vote into the orientation histogram.\n",
    "    \"\"\"\n",
    "\n",
    "    magnitude = sqrt(gx**2 + gy**2)\n",
    "    orientation = arctan2(gy, gx) * (180 / pi) % 180\n",
    "\n",
    "    sy, sx = flow.shape[:2]\n",
    "    cx, cy = pixels_per_cell\n",
    "    bx, by = cells_per_block\n",
    "\n",
    "    n_cellsx = int(np.floor(sx // cx))  # number of cells in x\n",
    "    n_cellsy = int(np.floor(sy // cy))  # number of cells in y\n",
    "\n",
    "    # compute orientations integral images\n",
    "    orientation_histogram = np.zeros((n_cellsy, n_cellsx, orientations))\n",
    "    subsample = np.index_exp[int(cy / 2):cy * n_cellsy:cy, int(cx / 2):cx * n_cellsx:cx]\n",
    "    # There are (orientations-1) bins for optical flow and 1 bin for no-motion\n",
    "    for i in range(orientations-1):\n",
    "        #create new integral image for this orientation\n",
    "        # isolate orientations in this range\n",
    "\n",
    "        # temp_ori = np.where(orientation < 180 / orientations * (i + 1), orientation, -1)\n",
    "        # temp_ori = np.where(orientation >= 180 / orientations * i, temp_ori, -1)\n",
    "        # fixed the bug in the original Github code\n",
    "        temp_ori = np.where(orientation < 180 / (orientations-1) * (i + 1), orientation, -1)\n",
    "        temp_ori = np.where(orientation >= 180 / (orientations-1) * i, temp_ori, -1)\n",
    "        # select magnitudes for those orientations\n",
    "        cond2 = (temp_ori > -1) * (magnitude > motion_threshold)\n",
    "        temp_mag = np.where(cond2, magnitude, 0)\n",
    "\n",
    "        temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n",
    "        orientation_histogram[:, :, i] = temp_filt[subsample]\n",
    "\n",
    "    ''' Calculate the no-motion bin '''\n",
    "    temp_mag = np.where(magnitude <= motion_threshold, magnitude, 0)\n",
    "\n",
    "    temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n",
    "    orientation_histogram[:, :, -1] = temp_filt[subsample]\n",
    "\n",
    "    \"\"\"\n",
    "    The fourth stage computes normalisation, which takes local groups of\n",
    "    cells and contrast normalises their overall responses before passing\n",
    "    to next stage. Normalisation introduces better invariance to illumination,\n",
    "    shadowing, and edge contrast. It is performed by accumulating a measure\n",
    "    of local histogram \"energy\" over local groups of cells that we call\n",
    "    \"blocks\". The result is used to normalise each cell in the block.\n",
    "    Typically each individual cell is shared between several blocks, but\n",
    "    its normalisations are block dependent and thus different. The cell\n",
    "    thus appears several times in the final output vector with different\n",
    "    normalisations. This may seem redundant but it improves the performance.\n",
    "    We refer to the normalised block descriptors as Histogram of Oriented\n",
    "    Gradient (hog) descriptors.\n",
    "    \"\"\"\n",
    "\n",
    "    n_blocksx = (n_cellsx - bx) + 1\n",
    "    n_blocksy = (n_cellsy - by) + 1\n",
    "    normalised_blocks = np.zeros((n_blocksy, n_blocksx,\n",
    "                                  by, bx, orientations))\n",
    "\n",
    "    for x in range(n_blocksx):\n",
    "        for y in range(n_blocksy):\n",
    "            block = orientation_histogram[y:y+by, x:x+bx, :]\n",
    "            eps = 1e-5\n",
    "            normalised_blocks[y, x, :] = block / sqrt(block.sum()**2 + eps)\n",
    "\n",
    "    return normalised_blocks.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "a4TF7vqH3ZCs"
   },
   "outputs": [],
   "source": [
    "# Define the HoF feature extraction function\n",
    "def extract_hof_feature(video_list):\n",
    "    feature_hof = []\n",
    "    label_list = []\n",
    "    img_width = 128\n",
    "    img_height = 64\n",
    "    for idx, value in enumerate(video_list):\n",
    "        # Display the progress\n",
    "        if (idx % 100) == 0:\n",
    "            print(\"process sequence %d/%d\" % (idx, len(video_list)))\n",
    "        filename = value[0]\n",
    "        label = value[1]\n",
    "        hof_feature_all = []\n",
    "\n",
    "        cap = cv2.VideoCapture(filename)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.resize(gray, (img_width, img_height)) # Resize frames to reduce feature dimensions\n",
    "        \n",
    "            while True:\n",
    "                previousGray = gray\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                if ret:\n",
    "                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                    gray = cv2.resize(gray, (img_width, img_height))\n",
    "                    flow = cv2.calcOpticalFlowFarneback(previousGray, gray, flow=None, pyr_scale=0.5, levels=5, winsize=11, iterations=10, poly_n=5, poly_sigma=1.1, flags=0)\n",
    "                    hof_feature_one = hof(flow, orientations=9, pixels_per_cell=(8, 8),cells_per_block=(2, 2))\n",
    "                    if (len(hof_feature_all) == 0):\n",
    "                        hof_feature_all = hof_feature_one\n",
    "                    else:\n",
    "                        hof_feature_all = np.vstack((hof_feature_all, hof_feature_one))\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "        cap.release()\n",
    "        if (len(hof_feature_all) != 0):\n",
    "            hof_feature_mean = np.mean(hof_feature_all, axis=0)\n",
    "            feature_hof.append(hof_feature_mean)\n",
    "            label_list.append(label)      \n",
    "        \n",
    "    return np.array(feature_hof), np.array(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CC0_4bBx3ZCu"
   },
   "outputs": [],
   "source": [
    "# It takes around half hour to prepare such feature dataset\n",
    "# You can uncomment if you want to re-build feature dataset, otherwise, load them from the data files\n",
    "# The sequence file basketball\\v_shooting_24\\v_shooting_24_01.avi is very short.\n",
    "\n",
    "# print(\"Prepare training feature dataset\")\n",
    "# train_feature_hof, train_label_hof = extract_hof_feature(ucf_train)\n",
    "# np.savez(\"data_train_hof_feature.npz\", X=train_feature_hof, Y=train_label_hof)\n",
    "# print(train_feature_hof.shape, train_label_hof.shape)\n",
    "\n",
    "# print(\"Prepare test feature dataset\")\n",
    "# test_feature_hof, test_label_hof = extract_hof_feature(ucf_test)\n",
    "# np.savez(\"data_test_hof_feature.npz\", X=test_feature_hof, Y=test_label_hof)\n",
    "# print(test_feature_hof.shape, test_label_hof.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5POsmAh3ZCx",
    "outputId": "cf728cbf-160d-4194-dc01-fdc1a667ee26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data (1293, 3780) (1293,)\n",
      "Test data (306, 3780) (306,)\n"
     ]
    }
   ],
   "source": [
    "# Load HoF features from pre-prepared data file and perform SVM classification\n",
    "with np.load(\"data_train_hof_feature.npz\") as npzfile:\n",
    "    x_train_hof = npzfile[\"X\"]\n",
    "    x_train_hof_label = npzfile[\"Y\"]\n",
    "    \n",
    "with np.load(\"data_test_hof_feature.npz\") as npzfile:\n",
    "    x_test_hof = npzfile[\"X\"]\n",
    "    x_test_hof_label = npzfile[\"Y\"]\n",
    "    \n",
    "print(\"Training data\", x_train_hof.shape, x_train_hof_label.shape)\n",
    "print(\"Test data\", x_test_hof.shape, x_test_hof_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-B_wJ3xB3ZC0",
    "outputId": "401c7f9e-3498-4e17-d6c7-f3ec1e20f550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  0  0  0  0  1  0  0  1]\n",
      " [ 4 34  0  0  4  1  1  0  0  0  1]\n",
      " [ 3  0  4  0  0  0  3  0  0  1  2]\n",
      " [ 0  0  2 24  1  1  3  1  0  3  1]\n",
      " [ 0  0  0  0 17  0  0  0  0  0  2]\n",
      " [ 6  0  0 10  0 12  0  2  1  1  0]\n",
      " [ 0  6  1  0  0  1 14  0  5  0  1]\n",
      " [ 5  0  1  5  4  5  0 24  3  3  2]\n",
      " [ 0  2  0  6  0  1  1  0 14  0  0]\n",
      " [ 3  0  5  1  2  0  0  1  0  4  2]\n",
      " [ 2  4  8  2  1  1  3  0  1  2 10]]\n"
     ]
    }
   ],
   "source": [
    "hof_svm_model = svm.SVC(kernel = 'linear', C = 10).fit(x_train_hof, x_train_hof_label)\n",
    "\n",
    "x_test_hof_pred = hof_svm_model.predict(x_test_hof)\n",
    "\n",
    "print(confusion_matrix(x_test_hof_label, x_test_hof_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEdUiVlb3ZC2"
   },
   "source": [
    "# Exercise 2: Action recognition using C3D model\n",
    "\n",
    "A modified C3D model is used in the workshop to reduce model training time for demonstration purpose.\n",
    "\n",
    "- Reference: D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, â€œLearning Spatiotemporal Features with 3D Convolutional Networksâ€œ, ICCV 2015, https://arxiv.org/abs/1412.0767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wijRprjc3ZC3"
   },
   "outputs": [],
   "source": [
    "class Videoto3D:\n",
    "\n",
    "    def __init__(self, width, height, depth):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.depth = depth\n",
    "\n",
    "    def get_data(self, filename, skip=True):\n",
    "        cap = cv2.VideoCapture(filename)\n",
    "        nframe = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        bAppend = False\n",
    "        if (nframe>=self.depth):\n",
    "            if skip:\n",
    "                frames = [x * nframe / self.depth for x in range(self.depth)]\n",
    "            else:\n",
    "                frames = [x for x in range(self.depth)]\n",
    "        else:\n",
    "            print(\"Insufficient %d frames in video %s, set bAppend as True\" % (nframe, filename))\n",
    "            bAppend = True\n",
    "            frames = [x for x in range(int(nframe))] # nframe is a float\n",
    "\n",
    "        framearray = []\n",
    "\n",
    "        for i in range(len(frames)):#self.depth):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frames[i])\n",
    "            ret, frame = cap.read()\n",
    "            frame = cv2.resize(frame, (self.height, self.width))\n",
    "            framearray.append(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "        cap.release()\n",
    "        \n",
    "        if bAppend:\n",
    "            while len(framearray) < self.depth:\n",
    "                framearray.append(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
    "            print(\"Append more frames in the framearray to have %d frames\" % len(framearray))\n",
    "                \n",
    "        return np.array(framearray)\n",
    "\n",
    "def loaddata(video_list, vid3d, skip=True):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for idx, value in enumerate(video_list):\n",
    "        # Display the progress\n",
    "        if (idx % 100) == 0:\n",
    "            print(\"process data %d/%d\" % (idx, len(video_list)))\n",
    "        filename = value[0]\n",
    "        label = value[1]\n",
    "        Y.append(label)\n",
    "        X.append(vid3d.get_data(filename, skip=skip))\n",
    "        \n",
    "    return np.array(X).transpose((0, 2, 3, 1)), np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NQQ8eVRf3ZC5"
   },
   "outputs": [],
   "source": [
    "# Define parameter setting\n",
    "class Args:\n",
    "    batch = 128\n",
    "    epoch = 50\n",
    "    nclass = 11 # 11 action categories\n",
    "    depth = 10\n",
    "    rows = 32\n",
    "    cols = 32\n",
    "    skip = True # Skip: randomly extract frames; otherwise, extract first few frames\n",
    "\n",
    "param_setting = Args()\n",
    "img_rows = param_setting.rows\n",
    "img_cols = param_setting.cols\n",
    "frames = param_setting.depth\n",
    "channel = 1\n",
    "vid3d = Videoto3D(img_rows, img_cols, frames)\n",
    "nb_classes = param_setting.nclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FD3kE9Rr3ZC8",
    "outputId": "bbdf9ed1-b903-4797-9a6b-2f2b2b81b90d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process data 0/1292\n",
      "process data 100/1292\n",
      "process data 200/1292\n",
      "process data 300/1292\n",
      "process data 400/1292\n",
      "process data 500/1292\n",
      "process data 600/1292\n",
      "process data 700/1292\n",
      "process data 800/1292\n",
      "process data 900/1292\n",
      "process data 1000/1292\n",
      "Insufficient 1 frames in video /content/gdrive/MyDrive/iss/RTAVS/action/data/basketball/v_shooting_24/v_shooting_24_01.avi, set bAppend as True\n",
      "Append more frames in the framearray to have 10 frames\n",
      "process data 1100/1292\n",
      "process data 1200/1292\n",
      "process data 0/308\n",
      "process data 100/308\n",
      "process data 200/308\n",
      "process data 300/308\n",
      "Training data (1292, 32, 32, 10, 1) (1292, 11)\n",
      "Test data (308, 32, 32, 10, 1) (308, 11)\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "x_train, y_train = loaddata(ucf_train, vid3d, param_setting.skip)\n",
    "x_train = x_train.reshape((x_train.shape[0], img_rows, img_cols, frames, channel))\n",
    "y_train = tf.keras.utils.to_categorical(y_train, nb_classes)  \n",
    "\n",
    "# Prepare test data\n",
    "x_test, y_test = loaddata(ucf_test, vid3d, param_setting.skip)\n",
    "x_test = x_test.reshape((x_test.shape[0], img_rows, img_cols, frames, channel))\n",
    "y_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print(\"Training data\", x_train.shape, y_train.shape)\n",
    "print(\"Test data\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcMdsP5w3ZC_",
    "outputId": "876c7bd1-f26f-43a6-ace5-a3ad701d679a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 32, 32, 10, 32)    896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 10, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 32, 32, 10, 32)    27680     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 10, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 11, 11, 4, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 11, 11, 4, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 11, 11, 4, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 11, 11, 4, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 11, 11, 4, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 11, 11, 4, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 4, 4, 2, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 2, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_feature (Flatten)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                5643      \n",
      "=================================================================\n",
      "Total params: 1,249,323\n",
      "Trainable params: 1,249,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define deep learning model\n",
    "# This is simplified C3D model\n",
    "c3d_model = Sequential()\n",
    "c3d_model.add(Conv3D(32, kernel_size=(3, 3, 3), input_shape=(x_train.shape[1:]), padding='same'))\n",
    "c3d_model.add(Activation('relu'))\n",
    "c3d_model.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n",
    "c3d_model.add(Activation('relu'))\n",
    "c3d_model.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "c3d_model.add(Dropout(0.2))\n",
    "\n",
    "c3d_model.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "c3d_model.add(Activation('relu'))\n",
    "c3d_model.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "c3d_model.add(Activation('relu'))\n",
    "c3d_model.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "c3d_model.add(Dropout(0.2))\n",
    "\n",
    "c3d_model.add(Flatten(name='flatten_feature'))\n",
    "c3d_model.add(Dense(512, activation='relu'))\n",
    "c3d_model.add(Dropout(0.2))\n",
    "c3d_model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "c3d_model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "c3d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpzMTZVk3ZDB",
    "outputId": "24bd01e6-3936-492c-a114-1ff69ab048ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11/11 - 36s - loss: 13.4044 - accuracy: 0.0960 - val_loss: 2.4069 - val_accuracy: 0.0487\n",
      "Epoch 2/50\n",
      "11/11 - 4s - loss: 2.3974 - accuracy: 0.1068 - val_loss: 2.3984 - val_accuracy: 0.0584\n",
      "Epoch 3/50\n",
      "11/11 - 4s - loss: 2.3955 - accuracy: 0.1432 - val_loss: 2.4002 - val_accuracy: 0.0584\n",
      "Epoch 4/50\n",
      "11/11 - 4s - loss: 2.3934 - accuracy: 0.1432 - val_loss: 2.4036 - val_accuracy: 0.0584\n",
      "Epoch 5/50\n",
      "11/11 - 4s - loss: 2.3904 - accuracy: 0.1385 - val_loss: 2.4075 - val_accuracy: 0.0584\n",
      "Epoch 6/50\n",
      "11/11 - 4s - loss: 2.3863 - accuracy: 0.1393 - val_loss: 2.4124 - val_accuracy: 0.0584\n",
      "Epoch 7/50\n",
      "11/11 - 4s - loss: 2.3783 - accuracy: 0.1401 - val_loss: 2.4202 - val_accuracy: 0.0584\n",
      "Epoch 8/50\n",
      "11/11 - 4s - loss: 2.3519 - accuracy: 0.1393 - val_loss: 2.4793 - val_accuracy: 0.0649\n",
      "Epoch 9/50\n",
      "11/11 - 4s - loss: 2.3169 - accuracy: 0.1703 - val_loss: 2.4511 - val_accuracy: 0.0682\n",
      "Epoch 10/50\n",
      "11/11 - 4s - loss: 2.2385 - accuracy: 0.2128 - val_loss: 2.4305 - val_accuracy: 0.0942\n",
      "Epoch 11/50\n",
      "11/11 - 4s - loss: 2.1374 - accuracy: 0.2554 - val_loss: 2.4212 - val_accuracy: 0.1591\n",
      "Epoch 12/50\n",
      "11/11 - 4s - loss: 2.0478 - accuracy: 0.3050 - val_loss: 2.3742 - val_accuracy: 0.1753\n",
      "Epoch 13/50\n",
      "11/11 - 4s - loss: 1.9894 - accuracy: 0.3282 - val_loss: 2.5763 - val_accuracy: 0.1786\n",
      "Epoch 14/50\n",
      "11/11 - 4s - loss: 1.9472 - accuracy: 0.3390 - val_loss: 2.4302 - val_accuracy: 0.2078\n",
      "Epoch 15/50\n",
      "11/11 - 4s - loss: 1.8538 - accuracy: 0.3847 - val_loss: 2.5451 - val_accuracy: 0.2435\n",
      "Epoch 16/50\n",
      "11/11 - 4s - loss: 1.7087 - accuracy: 0.4327 - val_loss: 2.1629 - val_accuracy: 0.3084\n",
      "Epoch 17/50\n",
      "11/11 - 4s - loss: 1.5720 - accuracy: 0.4969 - val_loss: 2.2993 - val_accuracy: 0.2857\n",
      "Epoch 18/50\n",
      "11/11 - 4s - loss: 1.5021 - accuracy: 0.4907 - val_loss: 2.2387 - val_accuracy: 0.3084\n",
      "Epoch 19/50\n",
      "11/11 - 4s - loss: 1.3990 - accuracy: 0.5348 - val_loss: 2.2784 - val_accuracy: 0.2792\n",
      "Epoch 20/50\n",
      "11/11 - 4s - loss: 1.3115 - accuracy: 0.5511 - val_loss: 2.2854 - val_accuracy: 0.3019\n",
      "Epoch 21/50\n",
      "11/11 - 4s - loss: 1.2157 - accuracy: 0.5782 - val_loss: 2.2252 - val_accuracy: 0.3571\n",
      "Epoch 22/50\n",
      "11/11 - 4s - loss: 1.1209 - accuracy: 0.6223 - val_loss: 2.4634 - val_accuracy: 0.2987\n",
      "Epoch 23/50\n",
      "11/11 - 4s - loss: 1.0766 - accuracy: 0.6324 - val_loss: 2.6559 - val_accuracy: 0.3084\n",
      "Epoch 24/50\n",
      "11/11 - 4s - loss: 1.1011 - accuracy: 0.6378 - val_loss: 2.3257 - val_accuracy: 0.3766\n",
      "Epoch 25/50\n",
      "11/11 - 4s - loss: 0.9966 - accuracy: 0.6757 - val_loss: 2.7997 - val_accuracy: 0.2987\n",
      "Epoch 26/50\n",
      "11/11 - 4s - loss: 0.8515 - accuracy: 0.7036 - val_loss: 2.6858 - val_accuracy: 0.3474\n",
      "Epoch 27/50\n",
      "11/11 - 4s - loss: 0.8169 - accuracy: 0.7159 - val_loss: 2.8797 - val_accuracy: 0.3084\n",
      "Epoch 28/50\n",
      "11/11 - 4s - loss: 0.7773 - accuracy: 0.7430 - val_loss: 2.8747 - val_accuracy: 0.3149\n",
      "Epoch 29/50\n",
      "11/11 - 4s - loss: 0.7916 - accuracy: 0.7322 - val_loss: 2.6758 - val_accuracy: 0.3279\n",
      "Epoch 30/50\n",
      "11/11 - 4s - loss: 0.7406 - accuracy: 0.7531 - val_loss: 2.9657 - val_accuracy: 0.2922\n",
      "Epoch 31/50\n",
      "11/11 - 4s - loss: 0.6053 - accuracy: 0.8011 - val_loss: 3.1478 - val_accuracy: 0.3474\n",
      "Epoch 32/50\n",
      "11/11 - 4s - loss: 0.6177 - accuracy: 0.8003 - val_loss: 3.3081 - val_accuracy: 0.3052\n",
      "Epoch 33/50\n",
      "11/11 - 4s - loss: 0.6919 - accuracy: 0.7678 - val_loss: 3.0709 - val_accuracy: 0.3214\n",
      "Epoch 34/50\n",
      "11/11 - 4s - loss: 0.6545 - accuracy: 0.7779 - val_loss: 3.0798 - val_accuracy: 0.3604\n",
      "Epoch 35/50\n",
      "11/11 - 4s - loss: 0.6135 - accuracy: 0.8104 - val_loss: 3.0859 - val_accuracy: 0.3636\n",
      "Epoch 36/50\n",
      "11/11 - 4s - loss: 0.4832 - accuracy: 0.8336 - val_loss: 3.2505 - val_accuracy: 0.3929\n",
      "Epoch 37/50\n",
      "11/11 - 4s - loss: 0.4072 - accuracy: 0.8615 - val_loss: 3.6667 - val_accuracy: 0.3961\n",
      "Epoch 38/50\n",
      "11/11 - 4s - loss: 0.3946 - accuracy: 0.8769 - val_loss: 3.5586 - val_accuracy: 0.3506\n",
      "Epoch 39/50\n",
      "11/11 - 4s - loss: 0.3030 - accuracy: 0.8963 - val_loss: 3.3580 - val_accuracy: 0.4026\n",
      "Epoch 40/50\n",
      "11/11 - 4s - loss: 0.2961 - accuracy: 0.9033 - val_loss: 4.0856 - val_accuracy: 0.3701\n",
      "Epoch 41/50\n",
      "11/11 - 4s - loss: 0.2296 - accuracy: 0.9265 - val_loss: 3.9371 - val_accuracy: 0.4123\n",
      "Epoch 42/50\n",
      "11/11 - 4s - loss: 0.2766 - accuracy: 0.8971 - val_loss: 4.3961 - val_accuracy: 0.3571\n",
      "Epoch 43/50\n",
      "11/11 - 4s - loss: 0.2626 - accuracy: 0.9071 - val_loss: 4.3108 - val_accuracy: 0.3896\n",
      "Epoch 44/50\n",
      "11/11 - 4s - loss: 0.3096 - accuracy: 0.8994 - val_loss: 4.2526 - val_accuracy: 0.3669\n",
      "Epoch 45/50\n",
      "11/11 - 4s - loss: 0.2813 - accuracy: 0.9048 - val_loss: 4.0088 - val_accuracy: 0.3734\n",
      "Epoch 46/50\n",
      "11/11 - 4s - loss: 0.3185 - accuracy: 0.8901 - val_loss: 4.3640 - val_accuracy: 0.4156\n",
      "Epoch 47/50\n",
      "11/11 - 4s - loss: 0.2695 - accuracy: 0.9071 - val_loss: 4.2449 - val_accuracy: 0.3864\n",
      "Epoch 48/50\n",
      "11/11 - 4s - loss: 0.2466 - accuracy: 0.9141 - val_loss: 4.8939 - val_accuracy: 0.3636\n",
      "Epoch 49/50\n",
      "11/11 - 4s - loss: 0.1925 - accuracy: 0.9373 - val_loss: 5.7396 - val_accuracy: 0.3571\n",
      "Epoch 50/50\n",
      "11/11 - 4s - loss: 0.1790 - accuracy: 0.9443 - val_loss: 4.7900 - val_accuracy: 0.4253\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "c3d_model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=param_setting.batch, epochs=param_setting.epoch, verbose=2, shuffle=True)\n",
    "\n",
    "c3d_model.save_weights(\"/content/gdrive/MyDrive/iss/RTAVS/action/data/model_c3d_v0815.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CkdwJtnYmD0D"
   },
   "outputs": [],
   "source": [
    "c3d_model.save('/content/gdrive/MyDrive/iss/RTAVS/action/data/model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfHPV5t93ZDD",
    "outputId": "1dc29b84-24a5-4102-a6f6-24125c1be168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 1  1  1  1  0  0  0  0  0  1  0]\n",
      " [ 0 15  2  1 13  3  6  1  2  1  3]\n",
      " [ 0  0  6  2  0  0  2  0  1  0  0]\n",
      " [ 2  1  0 26  1  0  0  1  1  2  0]\n",
      " [ 0  1  0  0 10  0  4  0  1  0  2]\n",
      " [ 1  0  0  8  0  0  6  6  6  0  6]\n",
      " [ 0  1  2  1  1  2 19  0  2  1  0]\n",
      " [ 9  1  1  0  1  7  0 27  1  0  5]\n",
      " [ 0  1  0  7  0  9  1  0  3  0  5]\n",
      " [ 1  0  0  3  0  0  1  0  3 10  0]\n",
      " [ 0  5  0  0  4  3  7  0  2  0 14]]\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "c3d_model.load_weights(\"/content/gdrive/MyDrive/iss/RTAVS/action/data/model_c3d_v0815.h5\")\n",
    "\n",
    "# Evaluate the deep learning model\n",
    "y_pred = c3d_model.predict(x_test, verbose=0)\n",
    "print(\"Confusion matrix\")\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWBBhV0w3ZDG"
   },
   "source": [
    "$\\color{red}{\\text{Q1: Complete code to perform action recognition using C3D + classifier}}$\n",
    "\n",
    "Tasks\n",
    "- Extract the fully connected layer response `flatten_feature` from the C3D model `c3d_model` as features from the training data `x_train`\n",
    "- Labels are provided in `ucf_train` and `ucf_test`\n",
    "- Build a classification model, such as SVM\n",
    "- Perform classification on `x_test`, display the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4W943NjK3ZDG"
   },
   "outputs": [],
   "source": [
    "# Provide your solution to Q1 here\n",
    "model =load_model('/content/gdrive/MyDrive/iss/RTAVS/action/data/model.hdf5')\n",
    "model.trainable = False\n",
    "flatten_layer_output = model.layers[-4].output\n",
    "\n",
    "new_model = Model(inputs=[model.input], outputs=flatten_layer_output)\n",
    "new_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOSFYlnHmOT7",
    "outputId": "4e9c103b-736c-440e-93fa-c44b97c6711e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_input (InputLayer)    [(None, 32, 32, 10, 1)]   0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 32, 32, 10, 32)    896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 10, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 32, 32, 10, 32)    27680     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 10, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 11, 11, 4, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 11, 11, 4, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 11, 11, 4, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 11, 11, 4, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 11, 11, 4, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 11, 11, 4, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 4, 4, 2, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 2, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_feature (Flatten)    (None, 2048)              0         \n",
      "=================================================================\n",
      "Total params: 194,592\n",
      "Trainable params: 0\n",
      "Non-trainable params: 194,592\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kGxDpFnhlQN",
    "outputId": "cfa9361f-1f2f-4d46-caef-73bcb2d72c27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare training feature dataset\n",
      "process data 0/1292\n",
      "process data 100/1292\n",
      "process data 200/1292\n",
      "process data 300/1292\n",
      "process data 400/1292\n",
      "process data 500/1292\n",
      "process data 600/1292\n",
      "process data 700/1292\n",
      "process data 800/1292\n",
      "process data 900/1292\n",
      "process data 1000/1292\n",
      "Insufficient 1 frames in video /content/gdrive/MyDrive/iss/RTAVS/action/data/basketball/v_shooting_24/v_shooting_24_01.avi, set bAppend as True\n",
      "Append more frames in the framearray to have 10 frames\n",
      "process data 1100/1292\n",
      "process data 1200/1292\n",
      "(1292, 2048) (1292,)\n",
      "Prepare test feature dataset\n",
      "process data 0/308\n",
      "process data 100/308\n",
      "process data 200/308\n",
      "process data 300/308\n",
      "(308, 2048) (308,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Prepare training feature dataset\")\n",
    "new_x_train = new_model.predict(x_train)\n",
    "_,new_y_train = loaddata(ucf_train, vid3d, param_setting.skip)\n",
    "print(new_x_train.shape, new_y_train.shape)\n",
    "\n",
    "print(\"Prepare test feature dataset\")\n",
    "new_x_test = new_model.predict(x_test)\n",
    "_,new_y_test = loaddata(ucf_test, vid3d, param_setting.skip)\n",
    "print(new_x_test.shape, new_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGawmDunhI6F",
    "outputId": "82ee66c4-5548-4d67-a2a7-5451cfe287a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  0  1  0  3  1  6  1  1  0]\n",
      " [ 1 14  0  0  4  0  3  0  3  0  7]\n",
      " [ 2  2 10  0  1  2  2  6  4  2  2]\n",
      " [ 0  0  0 24  0 17  0  9  2  1  0]\n",
      " [ 0 11  0  3 10  0  2  2  0  2  4]\n",
      " [ 0  7  0  2  0  1  4  3  8  0  3]\n",
      " [ 0  2  1  0  2  2 11  0  0  0  5]\n",
      " [ 0  0  0  1  0  4  0 24  1  0  0]\n",
      " [ 0  4  0  0  0  3  4  1  2  2  1]\n",
      " [ 1  1  0  2  0  0  1  0  1 10  0]\n",
      " [ 0  6  0  1  1  1  1  1  4  0 13]]\n"
     ]
    }
   ],
   "source": [
    "svm_model = svm.SVC(kernel = 'linear', C = 10).fit(new_x_train, new_y_train)\n",
    "\n",
    "new_y_pred = svm_model.predict(new_x_test)\n",
    "\n",
    "print(confusion_matrix(new_y_pred, new_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJbZZcKfuoo6"
   },
   "source": [
    "$\\color{red}{\\text{Q2: Propose how to apply the model developed in this workshop on the live video streaming.}}$\n",
    "\n",
    "The model developed in this workshop works on a short video clip. In practice, given a live video streaming, how to apply such model for action recognition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "q9wqi3V2un3x"
   },
   "outputs": [],
   "source": [
    "# Provide your solution to Q2 here (no need programming)\n",
    "# Take 2 cameras which capture live video stream\n",
    "# Suppose Camera 1 captures short video sequece. Meanwhile Camera 1 is processing sequence, Camera 2 can capture the scenario.\n",
    "# Similarly, when Camera 2 is processing, Camera 1 can capture.\n",
    "# Both cameras capture and process alternately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43UZE2MD3ZDI"
   },
   "source": [
    "**Once you finish the workshop, rename your .ipynb file to be your name, and submit your .ipynb file into LumiNUS.**\n",
    "\n",
    "Have a nice day!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "wk_action_keras_Apar Garg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
